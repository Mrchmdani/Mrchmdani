{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 31: Part 5: Advanced Statistical Methods in Python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 181. Introduction to Regression Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/10776980#content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Regression analysis is one of the most common methods of prediction. It is used whenever we have a causal relationship between variables. A great deal of predictive modeling in practice is done through regression analysis.\n",
    "    * ![Alt text](img/181.%20Introduction%20to%20Regression%20Analysis1.jpg)\n",
    "* It becomes extremely powerful when complimented by techniques like factor analysis, and you can truly find many academic papers based on it. Moreover, fundamentals of regression analysis are used in supervised machine learning. So you can see how regressions are a must for data science.\n",
    "    * ![Alt text](img/181.%20Introduction%20to%20Regression%20Analysis2.jpg)\n",
    "\n",
    "* Example:\n",
    "    * The general point is the following, among other factors, the amount of money you spend depends on the amount of money you earn.\n",
    "    * In the same way the amount of time you devote to this course is affected by your motivation to learn additional statistical methods. You can quantify these relationships and many others using regression analysis.\n",
    "        * * ![Alt text](img/181.%20Introduction%20to%20Regression%20Analysis3.jpg)\n",
    "* Typical step-by-step approach is start with a simple linear regression model, and before long, we'll be dealing with a multiple regression model.\n",
    "    * ![Alt text](img/181.%20Introduction%20to%20Regression%20Analysis4.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 107: Introduction to Regression Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Are you ready for regression analysis?\n",
    "    * Yes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 32: Advanced Statistical Methods - Linear Regression with StatsModels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 182. The Linear Regression Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/10776984#content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A linear regression is a linear approximation of a casual relationship between two or more variables.\n",
    "    * Regression models are highly valuable. As they are one of the most common ways to make inferences and predictions.\n",
    "* The Process:\n",
    "    * Get sample data.\n",
    "    * Design a model that explains the data.\n",
    "    * And then make predictions for the whole population.\n",
    "    * ![Alt text](img/182.%20The%20Linear%20Regression%20Model1.jpg)\n",
    "* There is a dependent variable, labeled 'Y' being predicted. And independent variables, labeled 'X1', 'X2', and so forth. These are the predictors. 'Y' is a function of the 'X' variables, and the regression model is a linear approximation of this function.\n",
    "    * ![Alt text](img/182.%20The%20Linear%20Regression%20Model2.jpg)\n",
    "* The easiest regression model is the simple linear regression.\n",
    "    * ![Alt text](img/182.%20The%20Linear%20Regression%20Model3.jpg)\n",
    "        * 'Y' is equal to beta zero plus beta one times 'X' plus epsilon.\n",
    "        * 'Y' is the variable we are trying to predict, and is called the dependent variable.\n",
    "        * 'X' is an independent variable.\n",
    "        * But to have a regression, 'Y' must depend on 'X' in some causal way. Whenever there is a change in 'X', such change must translate into a change in 'Y'.\n",
    "\n",
    "* Simple Linear Regression Equation\n",
    "    * ![Alt text](img/182.%20The%20Linear%20Regression%20Model4.jpg)\n",
    "    * The 'Y' here is referred to as 'Y' hat. Whenever we have a hat symbol, it is an estimated or predicted value. 'B' zero is the estimate of the regression constant Beta zero. While 'B' one is the estimate of Beta one and 'X' is the sample data for the independent variable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 108: The Linear Regression Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/quiz/4451910#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You have an ice-cream shop. You noticed a relationship between the number of cones you order and the number of ice-creams you sell. Is this a suitable situation for regression analysis?\n",
    "    * No\n",
    "    * While it is true that, if you run out of cones, you cannot sell anymore ice-creams, this is not regression analysis material. The two variables go hand-in-hand as (usually) each ice-cream requires a cone.\n",
    "2. You are trying to predict the amount of beer consumed in the US, depending on the state. Is this regression material?\n",
    "    * Yes\n",
    "    * Yes, logic shows us that, in different states, people drink different amounts of beer. Some states are warmer; other are colder etc. While many more things will be a part of this regression, such as gender, income etc., this is a good basis for regression analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 183. Correlation vs Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/10776986#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The relationship between correlation analysis and regression analysis.\n",
    "    * Correlation does not imply causation.\n",
    "        * First, correlation measures the degree of relationship between two variables. Regression analysis is about how one variable affects another, or what changes it causes to the other.\n",
    "        * Second, correlation doesn't capture causality but the degree of interrelation between the two variables. Regression is based on causality. It shows no degree of connection but cause and effect.\n",
    "        * Third, a property of correlation is that the correlation between x and y is the same as between y and x. This you can easily see from the formula which is symmetrical. Regressions of y on x and x on y yield different results. Think about our example with income and education. Predicting income based on education makes sense, but the opposite does not.\n",
    "        * Finally, the two methods have a very different graphical representation. Linear regression analysis is known for the best fitting line that goes through the data points and minimizes the distance between them, while correlation is a single point.\n",
    "    * ![Alt text](img/183.%20Correlation%20vs%20Regression1.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 109: Correlation vs Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/quiz/4451912#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Which statement is false?\n",
    "    * Correlation could be represted as a line\n",
    "    * Correlation is a single point. It cannot be represented as a line.\n",
    "        * What about statement that is True?\n",
    "            * Correlation does not imply causation\n",
    "            * Correlation is symmetrical regarding both variables\n",
    "            * Correlation does not capture the direction of the causal relationship"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 184. Geometrical Representation of the Linear Regression Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/10776988#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When we plot the data points on an XY plane the regression line is the best fitting line through the data points.\n",
    "    * ![Alt text](img/184.%20Geometrical%20Representation%20of%20the%20Linear%20Regression%20Model1.jpg)\n",
    "    * The gray points that are scattered are the observed values.\n",
    "    * B0 is a constant and is the intercept of the regression line with the Y axis.\n",
    "    * B1 is the slope of the regression line. It shows how much Y changes for each unit change of X.\n",
    "    * The distance between the observed values and the regression line is the estimator of the error term epsilon. It's point estimate is called residual.\n",
    "    * If you draw a perpendicular from an observed point to the regression line, the intercept between that perpendicular and the regression line is a point with a y value equal to Y hat, as we said earlier, given an x, Y hat is the value predicted by the regression line."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 110: Geometrical Representation of the Linear Regression Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/quiz/4545320#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Assume you have the following sample regression ŷ = 6 + x. If we draw the regression line, what would be its slope?\n",
    "    * 1\n",
    "    * The slope of the line is the coefficient in front of x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 185. Python Packages Installation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/10776992#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ![Alt text](img/185.%20Python%20Packages%20Installation1.jpg)\n",
    "* Python packages for data science\n",
    "    * NumPy\n",
    "        * Third party package allowing us to work with multidimensional arrays. They represent a powerful way to organize and process data. Because of that, NumPy is a fundamental library for those who attempt to manipulate larger chunks of information.\n",
    "    * Pandas\n",
    "        * package that enhances NumPy even further. It allows us to organize data in a tabular form and to attach descriptive labels to the rows and the columns of the table, not just numbers, as it is with NumPy. In addition, Pandas is endowed with a broad gamma of tools facilitating our work with various data formats and missing data. Therefore, if you wanna do data science with Python, Pandas is an essential package you will need.\n",
    "    * SciPy\n",
    "        * Python ecosystem containing a lot of tools for scientific calculations, suitable for the fields of mathematics, machine learning, engineering, and more.\n",
    "    * StatsModels\n",
    "        * another package built on top of NumPy and SciPy, which also integrates with Pandas. We will mainly create regressions in StatsModels, as they provide very good summaries, unmatched for educational purposes.\n",
    "    * Matplotlib\n",
    "        * two dimensional plotting library, specially designed for visualization of NumPy computations. In addition, it contains a large set of tools that can help you adjust a graph to your liking.\n",
    "    * Seaborn\n",
    "        * Python visualization library based on Matplotlib. It provides a high level interface for drawing attractive statistical graphics.\n",
    "    * scikit-learn\n",
    "        * one of the most widely-used machine learning libraries.\n",
    "* StatsModels vs Scikit-learn\n",
    "    * StatsModels to create and discuss regressions, but we will also provide the relevant code in sklearn. Think about it in the following way, sklearn is the real deal, but StatsModels makes it easier to understand the real deal. The final sections of the chorus will be based predominantly on sklearn."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Write in code for install\n",
    "    * pip install numpy\n",
    "    * pip install pandas\n",
    "    * pip install scipy\n",
    "    * pip install statsmodels\n",
    "    * pip install matplotlib\n",
    "    * pip install seaborn\n",
    "    * pip install scikit-learn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 186. First Regression in Python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/10776994#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAT</th>\n",
       "      <th>GPA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1714</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1664</td>\n",
       "      <td>2.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1760</td>\n",
       "      <td>2.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1685</td>\n",
       "      <td>2.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1693</td>\n",
       "      <td>2.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1936</td>\n",
       "      <td>3.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1810</td>\n",
       "      <td>3.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1987</td>\n",
       "      <td>3.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1962</td>\n",
       "      <td>3.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>2050</td>\n",
       "      <td>3.81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     SAT   GPA\n",
       "0   1714  2.40\n",
       "1   1664  2.52\n",
       "2   1760  2.54\n",
       "3   1685  2.74\n",
       "4   1693  2.83\n",
       "..   ...   ...\n",
       "79  1936  3.71\n",
       "80  1810  3.71\n",
       "81  1987  3.73\n",
       "82  1962  3.76\n",
       "83  2050  3.81\n",
       "\n",
       "[84 rows x 2 columns]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('csv/1.01. Simple linear regression.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAT</th>\n",
       "      <th>GPA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>84.000000</td>\n",
       "      <td>84.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1845.273810</td>\n",
       "      <td>3.330238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>104.530661</td>\n",
       "      <td>0.271617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1634.000000</td>\n",
       "      <td>2.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1772.000000</td>\n",
       "      <td>3.190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1846.000000</td>\n",
       "      <td>3.380000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1934.000000</td>\n",
       "      <td>3.502500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2050.000000</td>\n",
       "      <td>3.810000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               SAT        GPA\n",
       "count    84.000000  84.000000\n",
       "mean   1845.273810   3.330238\n",
       "std     104.530661   0.271617\n",
       "min    1634.000000   2.400000\n",
       "25%    1772.000000   3.190000\n",
       "50%    1846.000000   3.380000\n",
       "75%    1934.000000   3.502500\n",
       "max    2050.000000   3.810000"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()\n",
    "# sample 84 student\n",
    "# SAT = Total score of Critical Reading + Mathematics + Writing\n",
    "# GPA = Grade Point average (at graduation from University)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create first regression -> to predicts the GPA of a student based on SAT score."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Timeline:\n",
    "    * ![Alt text](img/186.%20First%20Regression%20in%20Python1.jpg)\n",
    "    * You sit the SAT and get a score. With this score, you apply to college. The next four years you attend college and graduate, receiving many grades forming your GPA, so that's the timeline.\n",
    "* ![Alt text](img/186.%20First%20Regression%20in%20Python2.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define the dependent and the independent variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Formula:\n",
    "    * ![Alt text](img/186.%20First%20Regression%20in%20Python3.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['GPA']\n",
    "x1 = data['SAT']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x1,y)\n",
    "plt.xlabel('SAT', fontsize=20)\n",
    "plt.ylabel('GPA', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Each point on the graph represents a different student. \n",
    "    * For instance, this is a student who scored around 1900 on the SAT and graduated with a 3.4 GPA. \n",
    "    * Observing all data points, we can see that there is a strong relationship between SAT and GPA. \n",
    "    * In general, the higher the SAT of a student, the higher their GPA."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Next, we need to create a new variable which call X.\n",
    "    * We have our X one, but we don't have an X zero. In fact, in the regression equation, there is no explicit X zero.\n",
    "    * The coefficient B zero is alone. That can be represented as B zero times one\n",
    "    * ![Alt text](img/186.%20First%20Regression%20in%20Python4.jpg)\n",
    "        * So if there was an X zero, it would always be one. It is really practical for computational purposes to incorporate this notion into the equation, and that's how we estimate the intercept B zero.\n",
    "* In terms of code, stats models uses the method add constant. So let's declare a new variable X equals SM dot add underscore constant X one.\n",
    "* Right after we do that, we will create another variable named results which will contain the output of the ordinary lease squares regression, or OLS.\n",
    "* As arguments, we must add the dependent variable Y and the newly defined x.\n",
    "* At the end, we will need the fit method, which you can think of as a method that will apply a specific estimation technique to obtain the fit of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>GPA</td>       <th>  R-squared:         </th> <td>   0.406</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.399</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   56.05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 26 Dec 2022</td> <th>  Prob (F-statistic):</th> <td>7.20e-11</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>19:48:24</td>     <th>  Log-Likelihood:    </th> <td>  12.672</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    84</td>      <th>  AIC:               </th> <td>  -21.34</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    82</td>      <th>  BIC:               </th> <td>  -16.48</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    0.2750</td> <td>    0.409</td> <td>    0.673</td> <td> 0.503</td> <td>   -0.538</td> <td>    1.088</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>SAT</th>   <td>    0.0017</td> <td>    0.000</td> <td>    7.487</td> <td> 0.000</td> <td>    0.001</td> <td>    0.002</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>12.839</td> <th>  Durbin-Watson:     </th> <td>   0.950</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.002</td> <th>  Jarque-Bera (JB):  </th> <td>  16.155</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.722</td> <th>  Prob(JB):          </th> <td>0.000310</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.590</td> <th>  Cond. No.          </th> <td>3.29e+04</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 3.29e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                    GPA   R-squared:                       0.406\n",
       "Model:                            OLS   Adj. R-squared:                  0.399\n",
       "Method:                 Least Squares   F-statistic:                     56.05\n",
       "Date:                Mon, 26 Dec 2022   Prob (F-statistic):           7.20e-11\n",
       "Time:                        19:48:24   Log-Likelihood:                 12.672\n",
       "No. Observations:                  84   AIC:                            -21.34\n",
       "Df Residuals:                      82   BIC:                            -16.48\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.2750      0.409      0.673      0.503      -0.538       1.088\n",
       "SAT            0.0017      0.000      7.487      0.000       0.001       0.002\n",
       "==============================================================================\n",
       "Omnibus:                       12.839   Durbin-Watson:                   0.950\n",
       "Prob(Omnibus):                  0.002   Jarque-Bera (JB):               16.155\n",
       "Skew:                          -0.722   Prob(JB):                     0.000310\n",
       "Kurtosis:                       4.590   Cond. No.                     3.29e+04\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 3.29e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sm.add_constant(x1)\n",
    "results = sm.OLS(y, x).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x1, y)\n",
    "yhat = 0.0017 * x1 + 0.275\n",
    "fig = plt.plot(x1, yhat, lw=4, c='orange', label='regression line')\n",
    "plt.xlabel('SAT', fontsize=20)\n",
    "plt.ylabel('GAP', fontsize=20)\n",
    "plt.show"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* based on regression line (orange) there is a strong relationship between SAT and GPA. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 187. First Regression in Python Exercise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/10887944#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 188. Using Seaborn for Graphs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/10776998#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* this lecture continue from section 32 no.186\n",
    "\n",
    "* Import seaborn as the conventional SNS and add the following line of code, sns.set(). \n",
    "    * This method overrides the style and the graphics of all matplotlib graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "plt.scatter(x1, y)\n",
    "yhat = 0.0017 * x1 + 0.275\n",
    "fig = plt.plot(x1, yhat, lw=4, c='orange', label='regression line')\n",
    "plt.xlabel('SAT', fontsize=20)\n",
    "plt.ylabel('GAP', fontsize=20)\n",
    "plt.show"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Difference matplotlib default style vs Seaborn\n",
    "    * ![Alt text](img/188.%20Using%20Seaborn%20for%20Graphs1.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 189. How to Interpret the Regression Table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/10777000#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* While the graphs we have seen so far are nice and easy to understand, when you perform regression analysis, you'll find something different than a scatter plot with a regression line.\n",
    "* The graph is a visual representation, and what we really want is the equation of the model and a measure of its significance and explanatory power.\n",
    "* This is why the regression summary consists of a few tables instead of a graph.\n",
    "* ![Alt text](img/189.%20How%20to%20Interpret%20the%20Regression%20Table1.jpg)\n",
    "    * When using stats models, we'll have three main tables,\n",
    "        * Model summary, \n",
    "        * Coefficients table\n",
    "            * Coefficient, constant (and bias) are used interchangeably.\n",
    "                * B0 = 0.275\n",
    "                * B1 =  0.0017\n",
    "                * Above number are the only two numbers we need to define the regression equation.\n",
    "                    * Next, created a variable called yhat. Yhat is equal to 0.0017, times x1, or SAT plus 0.275. That's the regression line.\n",
    "                    * Then, plot that line using the plot method. Can be picked from the coefficients table\n",
    "                        * Knowing that a person has scored 1700 on the SAT, we can substitute in the equation and obtain the following, 0.275 plus 0.0017, times 1700, which equals 3.165 (expected GPA for student). That's the predictive power of linear regressions.\n",
    "                    * ![Alt text](img/189.%20How%20to%20Interpret%20the%20Regression%20Table2.jpg)\n",
    "            * The standard error show the accuracy of prediction for each variable. **The lower the standard error, the better the estimate.**\n",
    "                * ![Alt text](img/189.%20How%20to%20Interpret%20the%20Regression%20Table3.jpg)\n",
    "            * T statistic and its P value for hypothesis. \n",
    "                * The null hypothesis of this test is, beta equals zero. In other words, is the coefficient equal to zero?\n",
    "                    * ![Alt text](img/189.%20How%20to%20Interpret%20the%20Regression%20Table4.jpg)\n",
    "                    * If a coefficient is zero for the intercept, B0 that is, then the line crosses the Y axis at the origin. \n",
    "                        * ![Alt text](img/189.%20How%20to%20Interpret%20the%20Regression%20Table5.jpg)\n",
    "                    * If beta one is zero then zero times x will always be zero for any x. So this variable will not be considered for the model. Graphically, that would mean that the regression line is horizontal, always going through the intercept value.\n",
    "                        * ![Alt text](img/189.%20How%20to%20Interpret%20the%20Regression%20Table6.jpg)\n",
    "            * Does it help us explain the variability we have in this case? The answer is contained in the P value column.\n",
    "                * **P value below 0.05 means that the variable is significant.** Therefore, the coefficient is most probably different from zero.\n",
    "                    * ![Alt text](img/189.%20How%20to%20Interpret%20the%20Regression%20Table7.jpg)\n",
    "                * So, what does SAT P value 0.000 mean? It simply tells us that **SAT score is a significant variable when predicting college GPA.**\n",
    "                    * ![Alt text](img/189.%20How%20to%20Interpret%20the%20Regression%20Table8.jpg)\n",
    "        * Additional tests."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 111: How to Interpret the Regression Table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/quiz/4545324#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What does a p-value of 0.503 suggest about the intercept coefficient?\n",
    "    * It is not significantly different from 0\n",
    "2. What does a p-value of 0.000 suggest about the coefficient (x)?\n",
    "    * It significantly different from 0\n",
    "3. Based on the regression table from the video: What is the predicted GPA of students with an SAT score of 1850? Note: Unlike in the lectures, this time assume that any coefficient with a p-value greater than 0.05 is not significantly different from 0\n",
    "    * 3.145"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 190. Decomposition of Variability"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/10777008#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ANOVA framework\n",
    "    * ![Alt text](img/190.%20Decomposition%20of%20Variability1.jpg)\n",
    "    * Sum of squares total (SST) or total sum of squares (TTS)\n",
    "        * SST is the square differences between the observed dependent variable and its mean. You could think of this as the dispersion of the observed variables around the mean, much like the variance we saw in descriptive statistics. It is a **measure of the total variability of the data set.**\n",
    "            * ![Alt text](img/190.%20Decomposition%20of%20Variability2.jpg)\n",
    "    * sum of squares regression (SSR) or explained sum of squares (ESS)\n",
    "        * SSR is the **sum of the differences between the predicted value and the mean of the dependent variable**. Think of it as a measure that describes how well your line fits the data. If this value of SSR is equal to the sum of squares total, it means your regression model captures all the observed variability and is perfect.\n",
    "            * ![Alt text](img/190.%20Decomposition%20of%20Variability3.jpg)\n",
    "    * sum of squares error (SSE) or residual sum of squares (RSS)\n",
    "        * The error is **the difference between the observed value and the predicted value**. The smaller the error, the better the estimation power of the regression.\n",
    "            * ![Alt text](img/190.%20Decomposition%20of%20Variability4.jpg)\n",
    "    * What is the connection among these three? Mathematically, SST is equal to SSR plus SSE. The total variability of the data set is equal to the variability explained by the regression line plus the unexplained variability known as error. Given a constant total variability, a lower error will cause a better regression. Conversely, a higher error will cause a less powerful regression.\n",
    "        * ![Alt text](img/190.%20Decomposition%20of%20Variability5.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 112: Decomposition of Variability"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/quiz/4451914#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Which of the following is true?\n",
    "    * SST = SSR + SSE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 191. What is the OLS?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/10777030#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ![Alt text](img/191.%20What%20is%20the%20OLS1.jpg)\n",
    "    * First, we have the dependent variable, or in other words the variable we are trying to predict. It's good to double check if you coded the regression properly through this cell.\n",
    "    * Next we have the model which is OLS or ordinary least squares. The method is closely related, least squares. In this case, there is no difference but later we will see discrepancies. \n",
    "        * So what is the OLS? **OLS or the ordinary least squares, is the most common method to estimate the linear regression equation.** Lease squares stands for the minimum squares error or SSE. Lower error results in a better explanatory power of the regression model. So this method aims to find the line which minimizes the sum of the squared errors.\n",
    "            * ![Alt text](img/191.%20What%20is%20the%20OLS2.jpg)\n",
    "            * ![Alt text](img/191.%20What%20is%20the%20OLS3.jpg)\n",
    "* There are other methods for determining the regression line. They are preferred in different contexts.\n",
    "    * ![Alt text](img/191.%20What%20is%20the%20OLS4.jpg)\n",
    "    * Generalized least squares,\n",
    "    * Maximum likelihood estimation, \n",
    "    * Bayesian regression,\n",
    "    * the Kernel regression and \n",
    "    * the Gaussian process regression.\n",
    "* However, the lease squares method is simple yet powerful enough for many, if not most linear problems."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 113: What is the OLS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/quiz/4545322#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Since OLS (Ordinary Least Squares) is simple enough to understand, why do advanced statisticians prefer using programming languages to solve regressions?\n",
    "    * Limitless capabilities and unmatched speed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 192. R-Squared"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/10777032#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The R-squared is an intuitive and practical tool, It is equal to variability explained by the regression divided by total variability.\n",
    "    * ![Alt text](img/192.%20R-Squared1.jpg)\n",
    "        * It is a relative measure and takes values ranging from zero to one. And R-squared of zero means your regression line explains none of the variability of the data. And R-squared of one, would mean your model explains the entire variability of the data. Unfortunately, regressions explaining the entire variability are rare. What you will usually observe is values ranging from 0.2 to 0.9.\n",
    "    * \"What is a good R-squared? When do I know for sure my regression is good enough?\"\n",
    "        * there is no definite answer to that. In fields such as physics and chemistry, scientists are usually looking for regressions with R-squared between 0.7 and 0.99. However, in social sciences such as economics, finance, and psychology, an R-squared of 0.2 or 20% of the variability explained by the model could be fantastic. **It depends on the complexity of the topic and how many variables are believed to be in play.**\n",
    "* The R-squared measures the goodness of fit of your model. The more factors you include in your regression, the higher the R-squared."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 114: R-Squared"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/quiz/4453050#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. SST = 1245, SSR = 945, SSE = 300. What is the R-squared of this regression?\n",
    "    * 0.76\n",
    "    * The R-squared is equal to SSR, divided by SST.\n",
    "2. The R-squared is a measure that: \n",
    "    * Measures how well your model fits your data\n",
    "    * The R-squared shows how much of the total variability of the dataset is explained by your regression model. This may be expressed as: how well your model fits your data. It is incorrect to say your regression line fits the data, as the line is the geometrical representation of the regression equation. It also incorrect to say the data fits the model or the regression line, as you are trying to explain the data with a model, not vice versa."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Section 33: Advanced Statitical Methods - Multiple Linear Regression with StatsModels**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 193. Multiple Linear Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/10777040#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Simple linear regression formula\n",
    "    * ![Alt text](img/193.%20Multiple%20Linear%20Regression1.jpg)\n",
    "    * Predicted the dependent variable Y with a single regressor X.\n",
    "* Multiple regressions address the higher complexity of problems. The more variables you have, the more factors you are considering in a model.\n",
    "    * ![Alt text](img/193.%20Multiple%20Linear%20Regression2.jpg)\n",
    "* The population multiple regression model. It is similar to a simple regression. The main difference is there are a more of independent variables, not just one.\n",
    "    * ![Alt text](img/193.%20Multiple%20Linear%20Regression3.jpg)\n",
    "        * Y hat is the inferred value? And B0 is the intercept. \n",
    "        * The independent variables range from X1 to XK. \n",
    "        * B1 to BK are their corresponding coefficients.\n",
    "    * The last thing to say about multiple regressions is that it's not about the best fitting line anymore. Actually, it stops being two dimensional and when we have over three dimensions, there is no visual way to represent the data.\n",
    "    * So if it is not about the line, what is it about? **It's about the best fitting model.**\n",
    "* how do we decrease the model's error?\n",
    "    * by increasing the explanatory power of the model. SSC and SSR are like communicating vessels. Each time we lower one, the other goes higher. With each additional variable, we increase the explanatory power by zero or more than zero, we cannot lower it. More variables usually equal a better fitting model.\n",
    "        * ![Alt text](img/193.%20Multiple%20Linear%20Regression4.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 115: Multiple Linear Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/quiz/4545328#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Why do we prefer using a multiple linear regression model to a simple linear regression model?\n",
    "    * More realistic - things often depend on 2, 3, 10 or even more factors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 194. Adjusted R-Squared"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/10777048#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When performing regression analysis a statistician would always take a look at adjusted R-squared.\n",
    "    * ![Alt text](img/194.%20Adjusted%20R-Squared1.jpg)\n",
    "        * The R-squared measures how much of the total variability is explained by our model.\n",
    "        * And two, multiple regressions are always better than simple ones, as with each additional variable that you add, the explanatory power may only increase or stay the same considering the number of variables.\n",
    "    * ![Alt text](img/194.%20Adjusted%20R-Squared2.jpg)\n",
    "        * The adjusted R-squared is always smaller than the R-squared, as it penalizes excessive use of variables."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAT</th>\n",
       "      <th>GPA</th>\n",
       "      <th>Rand 1,2,3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1714</td>\n",
       "      <td>2.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1664</td>\n",
       "      <td>2.52</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1760</td>\n",
       "      <td>2.54</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1685</td>\n",
       "      <td>2.74</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1693</td>\n",
       "      <td>2.83</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1936</td>\n",
       "      <td>3.71</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1810</td>\n",
       "      <td>3.71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1987</td>\n",
       "      <td>3.73</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1962</td>\n",
       "      <td>3.76</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>2050</td>\n",
       "      <td>3.81</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     SAT   GPA  Rand 1,2,3\n",
       "0   1714  2.40           1\n",
       "1   1664  2.52           3\n",
       "2   1760  2.54           3\n",
       "3   1685  2.74           3\n",
       "4   1693  2.83           2\n",
       "..   ...   ...         ...\n",
       "79  1936  3.71           3\n",
       "80  1810  3.71           1\n",
       "81  1987  3.73           3\n",
       "82  1962  3.76           1\n",
       "83  2050  3.81           2\n",
       "\n",
       "[84 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('csv/1.02. Multiple linear regression.csv')\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we have the same data with one additional variable called, random 1,2,3. That assigns one, two or three randomly to each student and 100% that this variable cannot predict college GPA, so this is our new model.\n",
    "* ![Alt text](img/194.%20Adjusted%20R-Squared3.jpg)\n",
    "    * College GPA is equal to B zero plus B one, times SAT score, plus B two, times the random variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAT</th>\n",
       "      <th>GPA</th>\n",
       "      <th>Rand 1,2,3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>84.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>84.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1845.273810</td>\n",
       "      <td>3.330238</td>\n",
       "      <td>2.059524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>104.530661</td>\n",
       "      <td>0.271617</td>\n",
       "      <td>0.855192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1634.000000</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1772.000000</td>\n",
       "      <td>3.190000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1846.000000</td>\n",
       "      <td>3.380000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1934.000000</td>\n",
       "      <td>3.502500</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2050.000000</td>\n",
       "      <td>3.810000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               SAT        GPA  Rand 1,2,3\n",
       "count    84.000000  84.000000   84.000000\n",
       "mean   1845.273810   3.330238    2.059524\n",
       "std     104.530661   0.271617    0.855192\n",
       "min    1634.000000   2.400000    1.000000\n",
       "25%    1772.000000   3.190000    1.000000\n",
       "50%    1846.000000   3.380000    2.000000\n",
       "75%    1934.000000   3.502500    3.000000\n",
       "max    2050.000000   3.810000    3.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#descriptive statistics\n",
    "data.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create multiple regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Y is GPA.\n",
    "* This time though, we have two explanatory variables, SAT and random 1, 2, 3.\n",
    "* What we can do is declare X one as a data frame containing both series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>GPA</td>       <th>  R-squared:         </th> <td>   0.407</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.392</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   27.76</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 27 Dec 2022</td> <th>  Prob (F-statistic):</th> <td>6.58e-10</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:00:30</td>     <th>  Log-Likelihood:    </th> <td>  12.720</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    84</td>      <th>  AIC:               </th> <td>  -19.44</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    81</td>      <th>  BIC:               </th> <td>  -12.15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>      <td>    0.2960</td> <td>    0.417</td> <td>    0.710</td> <td> 0.480</td> <td>   -0.533</td> <td>    1.125</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>SAT</th>        <td>    0.0017</td> <td>    0.000</td> <td>    7.432</td> <td> 0.000</td> <td>    0.001</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Rand 1,2,3</th> <td>   -0.0083</td> <td>    0.027</td> <td>   -0.304</td> <td> 0.762</td> <td>   -0.062</td> <td>    0.046</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>12.992</td> <th>  Durbin-Watson:     </th> <td>   0.948</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.002</td> <th>  Jarque-Bera (JB):  </th> <td>  16.364</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.731</td> <th>  Prob(JB):          </th> <td>0.000280</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.594</td> <th>  Cond. No.          </th> <td>3.33e+04</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 3.33e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                    GPA   R-squared:                       0.407\n",
       "Model:                            OLS   Adj. R-squared:                  0.392\n",
       "Method:                 Least Squares   F-statistic:                     27.76\n",
       "Date:                Tue, 27 Dec 2022   Prob (F-statistic):           6.58e-10\n",
       "Time:                        11:00:30   Log-Likelihood:                 12.720\n",
       "No. Observations:                  84   AIC:                            -19.44\n",
       "Df Residuals:                      81   BIC:                            -12.15\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.2960      0.417      0.710      0.480      -0.533       1.125\n",
       "SAT            0.0017      0.000      7.432      0.000       0.001       0.002\n",
       "Rand 1,2,3    -0.0083      0.027     -0.304      0.762      -0.062       0.046\n",
       "==============================================================================\n",
       "Omnibus:                       12.992   Durbin-Watson:                   0.948\n",
       "Prob(Omnibus):                  0.002   Jarque-Bera (JB):               16.364\n",
       "Skew:                          -0.731   Prob(JB):                     0.000280\n",
       "Kurtosis:                       4.594   Cond. No.                     3.33e+04\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 3.33e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data['GPA']\n",
    "x1 = data[['SAT', 'Rand 1,2,3']]\n",
    "\n",
    "x = sm.add_constant(x1)\n",
    "results = sm.OLS(y, x).fit()\n",
    "\n",
    "results.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* New vs Old table\n",
    "    * ![Alt text](img/194.%20Adjusted%20R-Squared4.jpg)\n",
    "        * We notice that the new R-squared is 0.407 so it seems as we have increased the explanatory power of the model, but then our enthusiasm is dampened by the adjusted R-squared of 0.392. We were penalized for adding an additional variable that had no strong explanatory power. We have added information but have lost value.\n",
    "            * Adding an impractical variable should be pointed out by the model in some way.\n",
    "        * We have determined a coefficient for the random 1, 2, 3 variable, but its p-value is 0.762. Remember the null hypothesis of the test? Beta two equals zero. **We cannot reject the null hypothesis at the 76% significance level.** This is an incredibly high p-value. For a coefficient to be statistically significant we want a p-value of less than 0.05.\n",
    "        * Our conclusion is :\n",
    "            * that the variable random 1, 2, 3 not only worsens the explanatory power of the model reflected by a lower adjusted R-squared but is also insignificant. \n",
    "            * Therefore, it should be dropped all together. Dropping useless variables is important. \n",
    "                * You can see the original model changed from Y hat equals 0.275 plus 0.0017 times x one. \n",
    "                * Two, Y hat equals 0.296 plus 0.0017 times x one minus 0.0083 times x two.\n",
    "            * The choice of third variable affected the intercept. Whenever you have one variable that is ruining the model you should not use this model altogether because the bias of this variable is reflected into the coefficients of the other variables. \n",
    "                * ![Alt text](img/194.%20Adjusted%20R-Squared5.jpg)\n",
    "            * **The correct approach is to remove it from the regression and run a new one, omitting the problematic predictor.**\n",
    "\n",
    "* The adjusted R-squared is the basis for comparing regression models.\n",
    "    * It only makes sense to compare two models considering the same dependent variable and using the same dataset. \n",
    "        * If we compare two models that are about two different dependent variables we will be making an apples to oranges comparison.\n",
    "        * If we use different data sets it is an apples to dinosaurs problem.\n",
    "            * ![Alt text](img/194.%20Adjusted%20R-Squared6.jpg)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 116: Adjusted R-Squared"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/quiz/4451922#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The adjusted R-squared is a measure that:\n",
    "    * measures how well your model fits the data but penalizes the excessive use of variables\n",
    "    * Like the R-squared, the adjusted R-squared measures how well your model fits the data. However, it penalizes the use of variables that are meaningless for the regression.\n",
    "2. The adjusted R-squared is:\n",
    "    * usually smaller than the R=squared\n",
    "    * Almost always, the adjusted R-squared is smaller than the R-squared. The statement is not true only in the extreme occasions of small sample sizes and a high number of independent variables.\n",
    "3. What can you tell about a new parameter if adding it increases R-squared but decreases adjusted R-squared\n",
    "    * The variable can be omitted since it holds no predictive power"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 195. Multiple Linear Regression Exercise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/10887972#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 196. Test for Significance of the Model (F-Test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/10777052#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Difference Z, T, and F statistics\n",
    "    * ![Alt text](img/196.%20Test%20for%20Significance%20of%20the%20Model%20(F-Test)1.jpg)\n",
    "        * the Z statistic that follows a normal distribution \n",
    "        * the T statistic that follows a student's T distribution\n",
    "        * the F statistic follows an F distribution.\n",
    "* The F statistics test is known as the test for overall significance of the model.\n",
    "    * The null hypothesis is all the betas are equal to zero simultaneously.\n",
    "    * The alternative hypothesis is at least one beta defers from zero.\n",
    "    * ![Alt text](img/196.%20Test%20for%20Significance%20of%20the%20Model%20(F-Test)2.jpg)\n",
    "    * What's the interpretation?\n",
    "        * If all betas are zero, then none of the independent variables matter, therefore our model has no merit.\n",
    "    * ![Alt text](img/196.%20Test%20for%20Significance%20of%20the%20Model%20(F-Test)3.jpg)\n",
    "        * The F statistic is 56.05. The cell below is its P value.\n",
    "        * The cell below is its P value. The number is really low, it is virtually 0.000.\n",
    "            * We say the overall model is significant. The F test is important for regressions as it gives us some important insights.\n",
    "    * Let's see the table for the model where we added the random 1, 2, 3 variable that had nothing to do with anything.\n",
    "        * ![Alt text](img/196.%20Test%20for%20Significance%20of%20the%20Model%20(F-Test)4.jpg)\n",
    "        * The F statistic is 27.76, and the P value is 0.000. \n",
    "            * You can see the F statistic is lower. The model is still significant but less so.\n",
    "        * The lower the F statistic, the closer to a non-significant model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 197. OLS Assumptions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/10777056#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Regression assumptions\n",
    "* ![Alt text](img/197.%20OLS%20Assumptions1.jpg)\n",
    "    * Linearity or Linear Regression\n",
    "        * Each independent variable is multiplied by a coefficient and summed up to predict the value.\n",
    "    * No Endogeneity of Regressors\n",
    "        * Mathematically, this is expressed as the covariance of the error, and the Xs is 0 for any error or X.\n",
    "    * Normality and homoscedasticity\n",
    "        * The expected value of the error is zero as we expect to have no errors on average.\n",
    "        * Homoscedasticity in plain English means **constant variance.**\n",
    "    * No autocorrelation\n",
    "        * Mathematically, the covariance of any two error terms is zero. \n",
    "        * That's the assumption that would usually stop you from using a linear regression in your analysis.\n",
    "        * No Multicollinearity\n",
    "            * Multicollinearity is observed when two or more variables have a high correlation between each other.\n",
    "\n",
    "* **At your workplace, the biggest mistake you can make is to perform a regression that violates one of these assumptions.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 117: OLS Assumptions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/quiz/4451924#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. If a regression assumption is violated:\n",
    "    * Performing regression analysis will yield an incorrect result.\n",
    "    * Nothing stops you from performing the regression analysis, but it will yield an incorrect result. It is a big deal."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 198. A1: Linearity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/10777058#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A linear regression is the simplest non-trivial relationship.\n",
    "    * Each independent variable is multiplied by a coefficient, and summed up to predict the value of the dependent variable.\n",
    "* How can you verify if the relationship between two variables is linear?\n",
    "    * The easiest way is to choose an independent variable x1, and plot it against the dependent y on a scatter plot.\n",
    "    * If the data points form a pattern that looks like a straight line, then a linear regression model is suitable.\n",
    "    * ![Alt text](img/198.%20A1%20Linearity1.jpg)\n",
    "\n",
    "* Case where the assumption is violated:\n",
    "    * After plotting another variable x2 against y on a scatter plot, we see there is no straight line that fits the data well. Actually, a curved line would be a very good fit. **Using a linear regression would not be appropriate.**\n",
    "        * ![Alt text](img/198.%20A1%20Linearity2.jpg)\n",
    "    * How to fixes:\n",
    "    * ![Alt text](img/198.%20A1%20Linearity3.jpg)\n",
    "        * Run a non-linear regression, as we will do in the next section, or transform your relationship.\n",
    "        * There are exponential and \n",
    "        * logarithmical transformations that help with that.\n",
    "    * The quadratic relationship we saw in this video could be easily transformed into a straight line with the appropriate methods. The takeaway is, if the relationship is non-linear, you should not use the data before transforming it appropriately."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 118: A1: Linearity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/quiz/4451926#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Linearity is easy to relax.\n",
    "    * True\n",
    "2. What should you do if you want to employ a linear regression but the relationship in your data is not linear?\n",
    "    * Transform it appropriately before using it"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 199. A2: No Endogeneity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/10777086#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* No Endogeneity of Regressors is refers to the prohibition of a link between the independent variables and the errors, mathematically expressed in the following way.\n",
    "    * ![Alt text](img/199.%20A2%20No%20Endogeneity1.jpg)\n",
    "    * The error, thus the difference between the observed values and the predicted values, is correlated with our independent values.\n",
    "    * **This is a problem referred to as omitted variable bias.**\n",
    "* Omitted variable bias is introduced to the model when you forget to include a relevant variable.\n",
    "    * As each independent variable explains y, they move together and are somewhat correlated.\n",
    "    * Similarly, y is also explained by the omitted variable, so they are also correlated.\n",
    "    * Chances are, the omitted variable is also correlated with at least one independent x.\n",
    "    * However, you forgot to include it as a regressor. Everything that you don't explain with your model goes into the error. Because of that, the error becomes correlated with everything else.\n",
    "        * ![Alt text](img/199.%20A2%20No%20Endogeneity2.jpg)\n",
    "\n",
    "* Omitted variable bias is hard to fix.\n",
    "    * ![Alt text](img/199.%20A2%20No%20Endogeneity3.jpg)\n",
    "    * We have only one variable, but when your model is exhaustive with 10 variables or more, you may feel disheartened.\n",
    "\n",
    "* Example:\n",
    "    * Why is bigger real estate cheaper?\n",
    "        * ![Alt text](img/199.%20A2%20No%20Endogeneity4.jpg)\n",
    "        * The sample comprises apartment buildings in central London, and is large. So the problem is not with the sample.\n",
    "        * What is it about smaller size that is making it so expensive? \n",
    "        * Where are the small houses? \n",
    "        * There is rarely construction of new apartment buildings in central London. And then you realize the City of London was in the sample. The place where most buildings are skyscrapers, with some of the most valuable real estate in the world. We omitted the exact location as a variable.\n",
    "        * After we included a variable that measures if the property is in London's City, everything falls into place. Larger properties are more expensive, and vice versa.\n",
    "    * The incorrect exclusion of a variable, like in this case, leads to biased and counterintuitive estimates that are toxic to your regression analysis.\n",
    "\n",
    "* Omitted variable bias is complex.\n",
    "    * It is always different, \n",
    "    * always sneaky, and \n",
    "    * only experience and advanced knowledge on the subject can help. \n",
    "    * Always check for it, and if you can't think of anything, ask a colleague for assistance.\n",
    "        * ![Alt text](img/199.%20A2%20No%20Endogeneity5.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 119: A2: No Endogeneity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/quiz/4451928#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The easiest way to detect an omitted variable bias is through:\n",
    "    * the error term\n",
    "    * Omitted variable bias occurs when you forget to include a variable. This is reflected in the error term as the factor you forgot about is included in the error. In this way, the error is not random but includes a systematic part (the omitted variable)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 200. A3: Normality and Homoscedasticity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/10777112#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ![Alt text](img/200.%20A3%20Normality%20and%20Homoscedasticity1.jpg)\n",
    "    * Normality\n",
    "        * ![Alt text](img/200.%20A3%20Normality%20and%20Homoscedasticity2.jpg)\n",
    "        * We assume the error term is normally distributed. Normal distribution is not required for creating the regression, but for making inferences.\n",
    "        * What should we do, if the error term, is not normally distributed?\n",
    "            * The central limit theorem. For large samples, the central limit theorem applies for the error terms too. Therefore, we can consider normality as a given for us. Therefore, we can consider normality as a given for us.\n",
    "    * zero mean\n",
    "        * ![Alt text](img/200.%20A3%20Normality%20and%20Homoscedasticity3.jpg)\n",
    "        * if the mean is not expected to be zero, then the line is not the best fitting one.\n",
    "        * However, having an intercept solves that problem. So in real life, it is unusual to violate this part of the assumption.\n",
    "    * homoscedasticity of the error term.\n",
    "        * ![Alt text](img/200.%20A3%20Normality%20and%20Homoscedasticity4.jpg)\n",
    "        * It means to have equal variance.\n",
    "        * So, the error term should have equal variance, one with the other.\n",
    "        * What if there was a pattern in the variance?\n",
    "            * an example of a data set, where errors have a different variance (heteroscedastic). Starting close to the regression line, and going further away. This would imply that, for smaller values, of the independent and dependent variables, we would have a better prediction than for bigger values. And, I assure you, we really don't like this uncertainty.\n",
    "* Prevention:3\n",
    "    * ![Alt text](img/200.%20A3%20Normality%20and%20Homoscedasticity5.jpg)\n",
    "    * First, check for omitted variable bias.\n",
    "    * After that, you can look for outliers, and try to remove them.\n",
    "    * the log transformation. Log stand for logarithm\n",
    "        * ![Alt text](img/200.%20A3%20Normality%20and%20Homoscedasticity6.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Example:\n",
    "    * ![Alt text](img/200.%20A3%20Normality%20and%20Homoscedasticity7.jpg)\n",
    "    * ![Alt text](img/200.%20A3%20Normality%20and%20Homoscedasticity8.jpg)\n",
    "        * On the left hand side of the chart, the variance of the error is small.\n",
    "        * While on the right, it is high.\n",
    "            * Here's the model. \n",
    "            * As X increases by one unit, Y grows, by b1 units.\n",
    "    * Transforming the X variable to a new variable, called log of x, and plot the data. Changing the scale of X would reduce the width of the graph.\n",
    "        * ![Alt text](img/200.%20A3%20Normality%20and%20Homoscedasticity9a.jpg)\n",
    "        * ![Alt text](img/200.%20A3%20Normality%20and%20Homoscedasticity9b.jpg)\n",
    "    * What if we transform the Y scale instead? Autologically, to what happened previously, we would expect the height of the graph to be reduced. This new model is also called semi-log model. It's meaning is, as X increases by one unit, Y changes, by b1 percent.\n",
    "        * ![Alt text](img/200.%20A3%20Normality%20and%20Homoscedasticity10a.jpg)\n",
    "        * ![Alt text](img/200.%20A3%20Normality%20and%20Homoscedasticity10b.jpg)\n",
    "    * Sometimes, we want or need to change, both scales to log. The result is a log-log model. We shrink the graph in height, and in width. The improvement is noticeable, but not game changing. However, we may be sure the assumption is not violated. The interpretation is, for each percentage point change in x, Y changes by b1 percentage points. If you've done economics, you would recognize such a relationship, is known as elasticity.\n",
    "        * ![Alt text](img/200.%20A3%20Normality%20and%20Homoscedasticity11a.jpg)\n",
    "        * ![Alt text](img/200.%20A3%20Normality%20and%20Homoscedasticity11b.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 201. A4: No Autocorrelation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/10777114#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* No Autocorrelation or no serial correlation\n",
    "    * Errors are assumed to be uncorrelated.\n",
    "    * Where can we observe serial correlation between errors? \n",
    "        * It is highly unlikely to find it in data taken at one moment of time, known as cross-sectional data. Very common in time series data.\n",
    "        * ![Alt text](img/201.%20A4%20No%20Autocorrelation1.jpg)\n",
    "* example: stock prices.\n",
    "    * ![Alt text](img/201.%20A4%20No%20Autocorrelation2.jpg)\n",
    "        * Every day you have a new quote for the same stock. These new numbers you see have the same underlying asset. \n",
    "        * Without getting too much into the finance, ideally you want them to be random or predicted by macro factors such as GDP, tax rate, political events and so on.\n",
    "        * Unfortunately, it is common in underdeveloped markets to see patterns in the stock prices.\n",
    "        * There is a well known phenomenon called The Day-of-the-Week Effect.\n",
    "    * The Day-of-the-Week Effect consists in disproportionately high returns on Fridays and low returns on Mondays. There is no consensus on the true nature of the day-of-the-week effect.\n",
    "        * One possible explanation proposed by Noble Prize winner Merton Miller is that investors don't have time to read all the news immediately.\n",
    "            * ![Alt text](img/201.%20A4%20No%20Autocorrelation3.jpg)\n",
    "            * So they do it over the weekend. \n",
    "                * The first day to respond to negative information is on Mondays.\n",
    "                * Then during the week, their advisors give them new positive information and they start buying on Thursdays and Fridays.\n",
    "        * Another famous explanation is given by the distinguished financier, Kenneth French, who suggested firms delay bad news for the weekends so markets react on Mondays.\n",
    "            * ![Alt text](img/201.%20A4%20No%20Autocorrelation4.jpg)\n",
    "        * Whatever the reason, there is correlation of the errors when building regressions about stock prices.\n",
    "            * The first observation, the sixth, the 11th and every fifth onwards would be Mondays.\n",
    "            * The fifth, 10th and so on would be Fridays. \n",
    "            * Errors on Mondays would be biased downwards and errors for Fridays would be biased upwards.\n",
    "            * The mathematics of the linear regression does not consider this. It assumes errors should be randomly spread around the regression line.\n",
    "\n",
    "* So how does one detect auto correlation?\n",
    "    * A common way is to plot all the residuals on a graph and look for patterns.\n",
    "        * ![Alt text](img/201.%20A4%20No%20Autocorrelation5.jpg)\n",
    "    * Another is the Durbin Watson test, which you have in the summary for the table provided by stats models.\n",
    "        * Generally, its values fall between zero and four. Two indicates no auto correlation, while values below one and above three are a cause for alarm.\n",
    "        * ![Alt text](img/201.%20A4%20No%20Autocorrelation6.jpg)\n",
    "\n",
    "* The only thing you can do is avoid using a linear regression in such a setting.\n",
    "    * There are other types of regressions that deal with time series data.\n",
    "        * auto aggress model,\n",
    "        * moving average model, \n",
    "        * auto regressive moving average model,\n",
    "        * auto regressive integrated moving average model.\n",
    "        * ![Alt text](img/201.%20A4%20No%20Autocorrelation7.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 120: A4: No autocorrelation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/quiz/4451932#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Autocorrelation is not likely to be observed in:\n",
    "    * cross-sectional data\n",
    "    * Autocorrelation is not observed in cross-sectional data. You usually spot it at time series data, which is a subset of panel data. Sample data is not relevant for this question.\n",
    "2. How do you fix autocorrelation when using a linear regression model?\n",
    "    * "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 202. A5: No Multicollinearity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/10777116#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* No Multicollinearity\n",
    "    * We observe multicollinearity when two or more variables have a high correlation.\n",
    "        * ![Alt text](img/202.%20A5%20No%20Multicollinearity1.jpg)\n",
    "            * A is equal to two plus five times B. A and B are two variables with an exact linear combination. \n",
    "            * A can be represented using B and B can be represented using A. \n",
    "            * In a model containing A and B, we would have perfect multicollinearity. \n",
    "            * This imposes a big problem to our regression model as the coefficients will be wrongly estimated.\n",
    "            * The reasoning is that, if A can be represented using B, there is no point using both.\n",
    "        * ![Alt text](img/202.%20A5%20No%20Multicollinearity2.jpg)\n",
    "            * Another example would be two variables, C and D, with a correlation of 90%.\n",
    "            * If we had a regression model using C and D, we would also have multicollinearity, although not perfect. \n",
    "            * Here, the assumption is still violated and poses a problem to our model.\n",
    "\n",
    "* How to fixes\n",
    "    * ![Alt text](img/202.%20A5%20No%20Multicollinearity3.jpg)\n",
    "        * The first one is to drop one of the two variables.\n",
    "        * The second is to transform them into one variable.\n",
    "        * keep them both while treating them with extreme caution.\n",
    "    * The correct approach depends on the research at hand.\n",
    "    * Multicollinearity is a big problem, but is also the easiest to notice.\n",
    "\n",
    "* Prevention:\n",
    "    * ![Alt text](img/202.%20A5%20No%20Multicollinearity4.jpg)\n",
    "    * Before creating the regression, find the correlation between each two pairs of independent variables and you will know if a multicollinearity problem may arise.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 121: A5: No Multicollinearity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/quiz/4451934#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. No multicollinearity is:\n",
    "    * easy to spot and easy to fix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 203. Dealing with Categorical Data - Dummy Variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/10777118#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In regression analysis, a dummy is a variable that is used to include categorical data into a regression model another meaning, an invitation or a copy that stands as a substitute.\n",
    "* Imitating the categories with numbers.\n",
    "    * Example:\n",
    "        * Gender:\n",
    "            * Male -> 0\n",
    "            * Female -> 1\n",
    "        * Attendance:\n",
    "            * No -> 0\n",
    "            * Yes -> 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels as sts\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAT</th>\n",
       "      <th>GPA</th>\n",
       "      <th>Attendance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1714</td>\n",
       "      <td>2.40</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1664</td>\n",
       "      <td>2.52</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1760</td>\n",
       "      <td>2.54</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1685</td>\n",
       "      <td>2.74</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1693</td>\n",
       "      <td>2.83</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1936</td>\n",
       "      <td>3.71</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1810</td>\n",
       "      <td>3.71</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1987</td>\n",
       "      <td>3.73</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1962</td>\n",
       "      <td>3.76</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>2050</td>\n",
       "      <td>3.81</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     SAT   GPA Attendance\n",
       "0   1714  2.40         No\n",
       "1   1664  2.52         No\n",
       "2   1760  2.54         No\n",
       "3   1685  2.74         No\n",
       "4   1693  2.83         No\n",
       "..   ...   ...        ...\n",
       "79  1936  3.71        Yes\n",
       "80  1810  3.71        Yes\n",
       "81  1987  3.73         No\n",
       "82  1962  3.76        Yes\n",
       "83  2050  3.81        Yes\n",
       "\n",
       "[84 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv('csv/1.03. Dummies.csv')\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAT</th>\n",
       "      <th>GPA</th>\n",
       "      <th>Attendance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1714</td>\n",
       "      <td>2.40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1664</td>\n",
       "      <td>2.52</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1760</td>\n",
       "      <td>2.54</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1685</td>\n",
       "      <td>2.74</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1693</td>\n",
       "      <td>2.83</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1936</td>\n",
       "      <td>3.71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1810</td>\n",
       "      <td>3.71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1987</td>\n",
       "      <td>3.73</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1962</td>\n",
       "      <td>3.76</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>2050</td>\n",
       "      <td>3.81</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     SAT   GPA  Attendance\n",
       "0   1714  2.40           0\n",
       "1   1664  2.52           0\n",
       "2   1760  2.54           0\n",
       "3   1685  2.74           0\n",
       "4   1693  2.83           0\n",
       "..   ...   ...         ...\n",
       "79  1936  3.71           1\n",
       "80  1810  3.71           1\n",
       "81  1987  3.73           0\n",
       "82  1962  3.76           1\n",
       "83  2050  3.81           1\n",
       "\n",
       "[84 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = raw_data.copy()\n",
    "\n",
    "data['Attendance'] = data['Attendance'].map({'Yes':1, 'No':0})\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 204. Dealing with Categorical Data - Dummy Variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 205. Making Predictions with the Linear Regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1b600a3438a79bc98971c3547d28f531d0c8ed6c91b96b1480a504fa30005dbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
