{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Section 31: Part 5: Advanced Statistical Methods in Python**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 181. Introduction to Regression Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/10776980#content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Regression analysis is one of the most common methods of prediction. It is used whenever we have a causal relationship between variables. A great deal of predictive modeling in practice is done through regression analysis.\n",
    "    * ![Alt text](img/181.%20Introduction%20to%20Regression%20Analysis1.jpg)\n",
    "* It becomes extremely powerful when complimented by techniques like factor analysis, and you can truly find many academic papers based on it. Moreover, fundamentals of regression analysis are used in supervised machine learning. So you can see how regressions are a must for data science.\n",
    "    * ![Alt text](img/181.%20Introduction%20to%20Regression%20Analysis2.jpg)\n",
    "\n",
    "* Example:\n",
    "    * The general point is the following, among other factors, the amount of money you spend depends on the amount of money you earn.\n",
    "    * In the same way the amount of time you devote to this course is affected by your motivation to learn additional statistical methods. You can quantify these relationships and many others using regression analysis.\n",
    "        * * ![Alt text](img/181.%20Introduction%20to%20Regression%20Analysis3.jpg)\n",
    "* Typical step-by-step approach is start with a simple linear regression model, and before long, we'll be dealing with a multiple regression model.\n",
    "    * ![Alt text](img/181.%20Introduction%20to%20Regression%20Analysis4.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 107: Introduction to Regression Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Are you ready for regression analysis?\n",
    "    * Yes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Section 32: Advanced Statistical Methods - Linear Regression with StatsModels**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 182. The Linear Regression Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/10776984#content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A linear regression is a linear approximation of a casual relationship between two or more variables.\n",
    "    * Regression models are highly valuable. As they are one of the most common ways to make inferences and predictions.\n",
    "* The Process:\n",
    "    * Get sample data.\n",
    "    * Design a model that explains the data.\n",
    "    * And then make predictions for the whole population.\n",
    "    * ![Alt text](img/182.%20The%20Linear%20Regression%20Model1.jpg)\n",
    "* There is a dependent variable, labeled 'Y' being predicted. And independent variables, labeled 'X1', 'X2', and so forth. These are the predictors. 'Y' is a function of the 'X' variables, and the regression model is a linear approximation of this function.\n",
    "    * ![Alt text](img/182.%20The%20Linear%20Regression%20Model2.jpg)\n",
    "* The easiest regression model is the simple linear regression.\n",
    "    * ![Alt text](img/182.%20The%20Linear%20Regression%20Model3.jpg)\n",
    "        * 'Y' is equal to beta zero plus beta one times 'X' plus epsilon.\n",
    "        * 'Y' is the variable we are trying to predict, and is called the dependent variable.\n",
    "        * 'X' is an independent variable.\n",
    "        * But to have a regression, 'Y' must depend on 'X' in some causal way. Whenever there is a change in 'X', such change must translate into a change in 'Y'.\n",
    "\n",
    "* Simple Linear Regression Equation\n",
    "    * ![Alt text](img/182.%20The%20Linear%20Regression%20Model4.jpg)\n",
    "    * The 'Y' here is referred to as 'Y' hat. Whenever we have a hat symbol, it is an estimated or predicted value. 'B' zero is the estimate of the regression constant Beta zero. While 'B' one is the estimate of Beta one and 'X' is the sample data for the independent variable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 108: The Linear Regression Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/quiz/4451910#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You have an ice-cream shop. You noticed a relationship between the number of cones you order and the number of ice-creams you sell. Is this a suitable situation for regression analysis?\n",
    "    * No\n",
    "    * While it is true that, if you run out of cones, you cannot sell anymore ice-creams, this is not regression analysis material. The two variables go hand-in-hand as (usually) each ice-cream requires a cone.\n",
    "2. You are trying to predict the amount of beer consumed in the US, depending on the state. Is this regression material?\n",
    "    * Yes\n",
    "    * Yes, logic shows us that, in different states, people drink different amounts of beer. Some states are warmer; other are colder etc. While many more things will be a part of this regression, such as gender, income etc., this is a good basis for regression analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 183. Correlation vs Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/10776986#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The relationship between correlation analysis and regression analysis.\n",
    "    * Correlation does not imply causation.\n",
    "        * First, correlation measures the degree of relationship between two variables. Regression analysis is about how one variable affects another, or what changes it causes to the other.\n",
    "        * Second, correlation doesn't capture causality but the degree of interrelation between the two variables. Regression is based on causality. It shows no degree of connection but cause and effect.\n",
    "        * Third, a property of correlation is that the correlation between x and y is the same as between y and x. This you can easily see from the formula which is symmetrical. Regressions of y on x and x on y yield different results. Think about our example with income and education. Predicting income based on education makes sense, but the opposite does not.\n",
    "        * Finally, the two methods have a very different graphical representation. Linear regression analysis is known for the best fitting line that goes through the data points and minimizes the distance between them, while correlation is a single point.\n",
    "    * ![Alt text](img/183.%20Correlation%20vs%20Regression1.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 109: Correlation vs Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/quiz/4451912#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Which statement is false?\n",
    "    * Correlation could be represted as a line\n",
    "    * Correlation is a single point. It cannot be represented as a line.\n",
    "        * What about statement that is True?\n",
    "            * Correlation does not imply causation\n",
    "            * Correlation is symmetrical regarding both variables\n",
    "            * Correlation does not capture the direction of the causal relationship"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 184. Geometrical Representation of the Linear Regression Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/10776988#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When we plot the data points on an XY plane the regression line is the best fitting line through the data points.\n",
    "    * ![Alt text](img/184.%20Geometrical%20Representation%20of%20the%20Linear%20Regression%20Model1.jpg)\n",
    "    * The gray points that are scattered are the observed values.\n",
    "    * B0 is a constant and is the intercept of the regression line with the Y axis.\n",
    "    * B1 is the slope of the regression line. It shows how much Y changes for each unit change of X.\n",
    "    * The distance between the observed values and the regression line is the estimator of the error term epsilon. It's point estimate is called residual.\n",
    "    * If you draw a perpendicular from an observed point to the regression line, the intercept between that perpendicular and the regression line is a point with a y value equal to Y hat, as we said earlier, given an x, Y hat is the value predicted by the regression line."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 110: Geometrical Representation of the Linear Regression Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/quiz/4545320#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Assume you have the following sample regression ŷ = 6 + x. If we draw the regression line, what would be its slope?\n",
    "    * 1\n",
    "    * The slope of the line is the coefficient in front of x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 185. Python Packages Installation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/10776992#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ![Alt text](img/185.%20Python%20Packages%20Installation1.jpg)\n",
    "* Python packages for data science\n",
    "    * NumPy\n",
    "        * Third party package allowing us to work with multidimensional arrays. They represent a powerful way to organize and process data. Because of that, NumPy is a fundamental library for those who attempt to manipulate larger chunks of information.\n",
    "    * Pandas\n",
    "        * package that enhances NumPy even further. It allows us to organize data in a tabular form and to attach descriptive labels to the rows and the columns of the table, not just numbers, as it is with NumPy. In addition, Pandas is endowed with a broad gamma of tools facilitating our work with various data formats and missing data. Therefore, if you wanna do data science with Python, Pandas is an essential package you will need.\n",
    "    * SciPy\n",
    "        * Python ecosystem containing a lot of tools for scientific calculations, suitable for the fields of mathematics, machine learning, engineering, and more.\n",
    "    * StatsModels\n",
    "        * another package built on top of NumPy and SciPy, which also integrates with Pandas. We will mainly create regressions in StatsModels, as they provide very good summaries, unmatched for educational purposes.\n",
    "    * Matplotlib\n",
    "        * two dimensional plotting library, specially designed for visualization of NumPy computations. In addition, it contains a large set of tools that can help you adjust a graph to your liking.\n",
    "    * Seaborn\n",
    "        * Python visualization library based on Matplotlib. It provides a high level interface for drawing attractive statistical graphics.\n",
    "    * scikit-learn\n",
    "        * one of the most widely-used machine learning libraries.\n",
    "* StatsModels vs Scikit-learn\n",
    "    * StatsModels to create and discuss regressions, but we will also provide the relevant code in sklearn. Think about it in the following way, sklearn is the real deal, but StatsModels makes it easier to understand the real deal. The final sections of the chorus will be based predominantly on sklearn."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Write in code for install\n",
    "    * pip install numpy\n",
    "    * pip install pandas\n",
    "    * pip install scipy\n",
    "    * pip install statsmodels\n",
    "    * pip install matplotlib\n",
    "    * pip install seaborn\n",
    "    * pip install scikit-learn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 186. First Regression in Python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/10776994#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAT</th>\n",
       "      <th>GPA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1714</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1664</td>\n",
       "      <td>2.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1760</td>\n",
       "      <td>2.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1685</td>\n",
       "      <td>2.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1693</td>\n",
       "      <td>2.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1936</td>\n",
       "      <td>3.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1810</td>\n",
       "      <td>3.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1987</td>\n",
       "      <td>3.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1962</td>\n",
       "      <td>3.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>2050</td>\n",
       "      <td>3.81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     SAT   GPA\n",
       "0   1714  2.40\n",
       "1   1664  2.52\n",
       "2   1760  2.54\n",
       "3   1685  2.74\n",
       "4   1693  2.83\n",
       "..   ...   ...\n",
       "79  1936  3.71\n",
       "80  1810  3.71\n",
       "81  1987  3.73\n",
       "82  1962  3.76\n",
       "83  2050  3.81\n",
       "\n",
       "[84 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('csv/1.01. Simple linear regression.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAT</th>\n",
       "      <th>GPA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>84.000000</td>\n",
       "      <td>84.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1845.273810</td>\n",
       "      <td>3.330238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>104.530661</td>\n",
       "      <td>0.271617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1634.000000</td>\n",
       "      <td>2.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1772.000000</td>\n",
       "      <td>3.190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1846.000000</td>\n",
       "      <td>3.380000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1934.000000</td>\n",
       "      <td>3.502500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2050.000000</td>\n",
       "      <td>3.810000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               SAT        GPA\n",
       "count    84.000000  84.000000\n",
       "mean   1845.273810   3.330238\n",
       "std     104.530661   0.271617\n",
       "min    1634.000000   2.400000\n",
       "25%    1772.000000   3.190000\n",
       "50%    1846.000000   3.380000\n",
       "75%    1934.000000   3.502500\n",
       "max    2050.000000   3.810000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()\n",
    "# sample 84 student\n",
    "# SAT = Total score of Critical Reading + Mathematics + Writing\n",
    "# GPA = Grade Point average (at graduation from University)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create first regression -> to predicts the GPA of a student based on SAT score."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Timeline:\n",
    "    * ![Alt text](img/186.%20First%20Regression%20in%20Python1.jpg)\n",
    "    * You sit the SAT and get a score. With this score, you apply to college. The next four years you attend college and graduate, receiving many grades forming your GPA, so that's the timeline.\n",
    "* ![Alt text](img/186.%20First%20Regression%20in%20Python2.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define the dependent and the independent variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Formula:\n",
    "    * ![Alt text](img/186.%20First%20Regression%20in%20Python3.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['GPA']\n",
    "x1 = data['SAT']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x1,y)\n",
    "plt.xlabel('SAT', fontsize=20)\n",
    "plt.ylabel('GPA', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Each point on the graph represents a different student. \n",
    "    * For instance, this is a student who scored around 1900 on the SAT and graduated with a 3.4 GPA. \n",
    "    * Observing all data points, we can see that there is a strong relationship between SAT and GPA. \n",
    "    * In general, the higher the SAT of a student, the higher their GPA."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Next, we need to create a new variable which call X.\n",
    "    * We have our X one, but we don't have an X zero. In fact, in the regression equation, there is no explicit X zero.\n",
    "    * The coefficient B zero is alone. That can be represented as B zero times one\n",
    "    * ![Alt text](img/186.%20First%20Regression%20in%20Python4.jpg)\n",
    "        * So if there was an X zero, it would always be one. It is really practical for computational purposes to incorporate this notion into the equation, and that's how we estimate the intercept B zero.\n",
    "* In terms of code, stats models uses the method add constant. So let's declare a new variable X equals SM dot add underscore constant X one.\n",
    "* Right after we do that, we will create another variable named results which will contain the output of the ordinary lease squares regression, or OLS.\n",
    "* As arguments, we must add the dependent variable Y and the newly defined x.\n",
    "* At the end, we will need the fit method, which you can think of as a method that will apply a specific estimation technique to obtain the fit of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>GPA</td>       <th>  R-squared:         </th> <td>   0.406</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.399</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   56.05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 28 Dec 2022</td> <th>  Prob (F-statistic):</th> <td>7.20e-11</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:22:52</td>     <th>  Log-Likelihood:    </th> <td>  12.672</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    84</td>      <th>  AIC:               </th> <td>  -21.34</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    82</td>      <th>  BIC:               </th> <td>  -16.48</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    0.2750</td> <td>    0.409</td> <td>    0.673</td> <td> 0.503</td> <td>   -0.538</td> <td>    1.088</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>SAT</th>   <td>    0.0017</td> <td>    0.000</td> <td>    7.487</td> <td> 0.000</td> <td>    0.001</td> <td>    0.002</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>12.839</td> <th>  Durbin-Watson:     </th> <td>   0.950</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.002</td> <th>  Jarque-Bera (JB):  </th> <td>  16.155</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.722</td> <th>  Prob(JB):          </th> <td>0.000310</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.590</td> <th>  Cond. No.          </th> <td>3.29e+04</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 3.29e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                    GPA   R-squared:                       0.406\n",
       "Model:                            OLS   Adj. R-squared:                  0.399\n",
       "Method:                 Least Squares   F-statistic:                     56.05\n",
       "Date:                Wed, 28 Dec 2022   Prob (F-statistic):           7.20e-11\n",
       "Time:                        21:22:52   Log-Likelihood:                 12.672\n",
       "No. Observations:                  84   AIC:                            -21.34\n",
       "Df Residuals:                      82   BIC:                            -16.48\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.2750      0.409      0.673      0.503      -0.538       1.088\n",
       "SAT            0.0017      0.000      7.487      0.000       0.001       0.002\n",
       "==============================================================================\n",
       "Omnibus:                       12.839   Durbin-Watson:                   0.950\n",
       "Prob(Omnibus):                  0.002   Jarque-Bera (JB):               16.155\n",
       "Skew:                          -0.722   Prob(JB):                     0.000310\n",
       "Kurtosis:                       4.590   Cond. No.                     3.29e+04\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 3.29e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sm.add_constant(x1)\n",
    "results = sm.OLS(y, x).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x1, y)\n",
    "yhat = 0.0017 * x1 + 0.275\n",
    "fig = plt.plot(x1, yhat, lw=4, c='orange', label='regression line')\n",
    "plt.xlabel('SAT', fontsize=20)\n",
    "plt.ylabel('GAP', fontsize=20)\n",
    "plt.show"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* based on regression line (orange) there is a strong relationship between SAT and GPA. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 187. First Regression in Python Exercise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/10887944#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 188. Using Seaborn for Graphs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/10776998#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* this lecture continue from section 32 no.186\n",
    "\n",
    "* Import seaborn as the conventional SNS and add the following line of code, sns.set(). \n",
    "    * This method overrides the style and the graphics of all matplotlib graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "plt.scatter(x1, y)\n",
    "yhat = 0.0017 * x1 + 0.275\n",
    "fig = plt.plot(x1, yhat, lw=4, c='orange', label='regression line')\n",
    "plt.xlabel('SAT', fontsize=20)\n",
    "plt.ylabel('GAP', fontsize=20)\n",
    "plt.show"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Difference matplotlib default style vs Seaborn\n",
    "    * ![Alt text](img/188.%20Using%20Seaborn%20for%20Graphs1.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 189. How to Interpret the Regression Table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/10777000#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* While the graphs we have seen so far are nice and easy to understand, when you perform regression analysis, you'll find something different than a scatter plot with a regression line.\n",
    "* The graph is a visual representation, and what we really want is the equation of the model and a measure of its significance and explanatory power.\n",
    "* This is why the regression summary consists of a few tables instead of a graph.\n",
    "* ![Alt text](img/189.%20How%20to%20Interpret%20the%20Regression%20Table1.jpg)\n",
    "    * When using stats models, we'll have three main tables,\n",
    "        * Model summary, \n",
    "        * Coefficients table\n",
    "            * Coefficient, constant (and bias) are used interchangeably.\n",
    "                * B0 = 0.275\n",
    "                * B1 =  0.0017\n",
    "                * Above number are the only two numbers we need to define the regression equation.\n",
    "                    * Next, created a variable called yhat. Yhat is equal to 0.0017, times x1, or SAT plus 0.275. That's the regression line.\n",
    "                    * Then, plot that line using the plot method. Can be picked from the coefficients table\n",
    "                        * Knowing that a person has scored 1700 on the SAT, we can substitute in the equation and obtain the following, 0.275 plus 0.0017, times 1700, which equals 3.165 (expected GPA for student). That's the predictive power of linear regressions.\n",
    "                    * ![Alt text](img/189.%20How%20to%20Interpret%20the%20Regression%20Table2.jpg)\n",
    "            * The standard error show the accuracy of prediction for each variable. **The lower the standard error, the better the estimate.**\n",
    "                * ![Alt text](img/189.%20How%20to%20Interpret%20the%20Regression%20Table3.jpg)\n",
    "            * T statistic and its P value for hypothesis. \n",
    "                * The null hypothesis of this test is, beta equals zero. In other words, is the coefficient equal to zero?\n",
    "                    * ![Alt text](img/189.%20How%20to%20Interpret%20the%20Regression%20Table4.jpg)\n",
    "                    * If a coefficient is zero for the intercept, B0 that is, then the line crosses the Y axis at the origin. \n",
    "                        * ![Alt text](img/189.%20How%20to%20Interpret%20the%20Regression%20Table5.jpg)\n",
    "                    * If beta one is zero then zero times x will always be zero for any x. So this variable will not be considered for the model. Graphically, that would mean that the regression line is horizontal, always going through the intercept value.\n",
    "                        * ![Alt text](img/189.%20How%20to%20Interpret%20the%20Regression%20Table6.jpg)\n",
    "            * Does it help us explain the variability we have in this case? The answer is contained in the P value column.\n",
    "                * **P value below 0.05 means that the variable is significant.** Therefore, the coefficient is most probably different from zero.\n",
    "                    * ![Alt text](img/189.%20How%20to%20Interpret%20the%20Regression%20Table7.jpg)\n",
    "                * So, what does SAT P value 0.000 mean? It simply tells us that **SAT score is a significant variable when predicting college GPA.**\n",
    "                    * ![Alt text](img/189.%20How%20to%20Interpret%20the%20Regression%20Table8.jpg)\n",
    "        * Additional tests."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 111: How to Interpret the Regression Table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/quiz/4545324#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What does a p-value of 0.503 suggest about the intercept coefficient?\n",
    "    * It is not significantly different from 0\n",
    "2. What does a p-value of 0.000 suggest about the coefficient (x)?\n",
    "    * It significantly different from 0\n",
    "3. Based on the regression table from the video: What is the predicted GPA of students with an SAT score of 1850? Note: Unlike in the lectures, this time assume that any coefficient with a p-value greater than 0.05 is not significantly different from 0\n",
    "    * 3.145"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 190. Decomposition of Variability"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/10777008#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ANOVA framework\n",
    "    * ![Alt text](img/190.%20Decomposition%20of%20Variability1.jpg)\n",
    "    * Sum of squares total (SST) or total sum of squares (TTS)\n",
    "        * SST is the square differences between the observed dependent variable and its mean. You could think of this as the dispersion of the observed variables around the mean, much like the variance we saw in descriptive statistics. It is a **measure of the total variability of the data set.**\n",
    "            * ![Alt text](img/190.%20Decomposition%20of%20Variability2.jpg)\n",
    "    * sum of squares regression (SSR) or explained sum of squares (ESS)\n",
    "        * SSR is the **sum of the differences between the predicted value and the mean of the dependent variable**. Think of it as a measure that describes how well your line fits the data. If this value of SSR is equal to the sum of squares total, it means your regression model captures all the observed variability and is perfect.\n",
    "            * ![Alt text](img/190.%20Decomposition%20of%20Variability3.jpg)\n",
    "    * sum of squares error (SSE) or residual sum of squares (RSS)\n",
    "        * The error is **the difference between the observed value and the predicted value**. The smaller the error, the better the estimation power of the regression.\n",
    "            * ![Alt text](img/190.%20Decomposition%20of%20Variability4.jpg)\n",
    "    * What is the connection among these three? Mathematically, SST is equal to SSR plus SSE. The total variability of the data set is equal to the variability explained by the regression line plus the unexplained variability known as error. Given a constant total variability, a lower error will cause a better regression. Conversely, a higher error will cause a less powerful regression.\n",
    "        * ![Alt text](img/190.%20Decomposition%20of%20Variability5.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 112: Decomposition of Variability"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/quiz/4451914#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Which of the following is true?\n",
    "    * SST = SSR + SSE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 191. What is the OLS?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/10777030#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ![Alt text](img/191.%20What%20is%20the%20OLS1.jpg)\n",
    "    * First, we have the dependent variable, or in other words the variable we are trying to predict. It's good to double check if you coded the regression properly through this cell.\n",
    "    * Next we have the model which is OLS or ordinary least squares. The method is closely related, least squares. In this case, there is no difference but later we will see discrepancies. \n",
    "        * So what is the OLS? **OLS or the ordinary least squares, is the most common method to estimate the linear regression equation.** Lease squares stands for the minimum squares error or SSE. Lower error results in a better explanatory power of the regression model. So this method aims to find the line which minimizes the sum of the squared errors.\n",
    "            * ![Alt text](img/191.%20What%20is%20the%20OLS2.jpg)\n",
    "            * ![Alt text](img/191.%20What%20is%20the%20OLS3.jpg)\n",
    "* There are other methods for determining the regression line. They are preferred in different contexts.\n",
    "    * ![Alt text](img/191.%20What%20is%20the%20OLS4.jpg)\n",
    "    * Generalized least squares,\n",
    "    * Maximum likelihood estimation, \n",
    "    * Bayesian regression,\n",
    "    * the Kernel regression and \n",
    "    * the Gaussian process regression.\n",
    "* However, the lease squares method is simple yet powerful enough for many, if not most linear problems."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 113: What is the OLS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/quiz/4545322#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Since OLS (Ordinary Least Squares) is simple enough to understand, why do advanced statisticians prefer using programming languages to solve regressions?\n",
    "    * Limitless capabilities and unmatched speed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 192. R-Squared"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/10777032#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The R-squared is an intuitive and practical tool, It is equal to variability explained by the regression divided by total variability.\n",
    "    * ![Alt text](img/192.%20R-Squared1.jpg)\n",
    "        * It is a relative measure and takes values ranging from zero to one. And R-squared of zero means your regression line explains none of the variability of the data. And R-squared of one, would mean your model explains the entire variability of the data. Unfortunately, regressions explaining the entire variability are rare. What you will usually observe is values ranging from 0.2 to 0.9.\n",
    "    * \"What is a good R-squared? When do I know for sure my regression is good enough?\"\n",
    "        * there is no definite answer to that. In fields such as physics and chemistry, scientists are usually looking for regressions with R-squared between 0.7 and 0.99. However, in social sciences such as economics, finance, and psychology, an R-squared of 0.2 or 20% of the variability explained by the model could be fantastic. **It depends on the complexity of the topic and how many variables are believed to be in play.**\n",
    "* The R-squared measures the goodness of fit of your model. The more factors you include in your regression, the higher the R-squared."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 114: R-Squared"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/quiz/4453050#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. SST = 1245, SSR = 945, SSE = 300. What is the R-squared of this regression?\n",
    "    * 0.76\n",
    "    * The R-squared is equal to SSR, divided by SST.\n",
    "2. The R-squared is a measure that: \n",
    "    * Measures how well your model fits your data\n",
    "    * The R-squared shows how much of the total variability of the dataset is explained by your regression model. This may be expressed as: how well your model fits your data. It is incorrect to say your regression line fits the data, as the line is the geometrical representation of the regression equation. It also incorrect to say the data fits the model or the regression line, as you are trying to explain the data with a model, not vice versa."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Section 33: Advanced Statitical Methods - Multiple Linear Regression with StatsModels**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 193. Multiple Linear Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/10777040#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Simple linear regression formula\n",
    "    * ![Alt text](img/193.%20Multiple%20Linear%20Regression1.jpg)\n",
    "    * Predicted the dependent variable Y with a single regressor X.\n",
    "* Multiple regressions address the higher complexity of problems. The more variables you have, the more factors you are considering in a model.\n",
    "    * ![Alt text](img/193.%20Multiple%20Linear%20Regression2.jpg)\n",
    "* The population multiple regression model. It is similar to a simple regression. The main difference is there are a more of independent variables, not just one.\n",
    "    * ![Alt text](img/193.%20Multiple%20Linear%20Regression3.jpg)\n",
    "        * Y hat is the inferred value? And B0 is the intercept. \n",
    "        * The independent variables range from X1 to XK. \n",
    "        * B1 to BK are their corresponding coefficients.\n",
    "    * The last thing to say about multiple regressions is that it's not about the best fitting line anymore. Actually, it stops being two dimensional and when we have over three dimensions, there is no visual way to represent the data.\n",
    "    * So if it is not about the line, what is it about? **It's about the best fitting model.**\n",
    "* how do we decrease the model's error?\n",
    "    * by increasing the explanatory power of the model. SSC and SSR are like communicating vessels. Each time we lower one, the other goes higher. With each additional variable, we increase the explanatory power by zero or more than zero, we cannot lower it. More variables usually equal a better fitting model.\n",
    "        * ![Alt text](img/193.%20Multiple%20Linear%20Regression4.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 115: Multiple Linear Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/quiz/4545328#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Why do we prefer using a multiple linear regression model to a simple linear regression model?\n",
    "    * More realistic - things often depend on 2, 3, 10 or even more factors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 194. Adjusted R-Squared"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/10777048#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When performing regression analysis a statistician would always take a look at adjusted R-squared.\n",
    "    * ![Alt text](img/194.%20Adjusted%20R-Squared1.jpg)\n",
    "        * The R-squared measures how much of the total variability is explained by our model.\n",
    "        * And two, multiple regressions are always better than simple ones, as with each additional variable that you add, the explanatory power may only increase or stay the same considering the number of variables.\n",
    "    * ![Alt text](img/194.%20Adjusted%20R-Squared2.jpg)\n",
    "        * The adjusted R-squared is always smaller than the R-squared, as it penalizes excessive use of variables."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAT</th>\n",
       "      <th>GPA</th>\n",
       "      <th>Rand 1,2,3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1714</td>\n",
       "      <td>2.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1664</td>\n",
       "      <td>2.52</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1760</td>\n",
       "      <td>2.54</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1685</td>\n",
       "      <td>2.74</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1693</td>\n",
       "      <td>2.83</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1936</td>\n",
       "      <td>3.71</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1810</td>\n",
       "      <td>3.71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1987</td>\n",
       "      <td>3.73</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1962</td>\n",
       "      <td>3.76</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>2050</td>\n",
       "      <td>3.81</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     SAT   GPA  Rand 1,2,3\n",
       "0   1714  2.40           1\n",
       "1   1664  2.52           3\n",
       "2   1760  2.54           3\n",
       "3   1685  2.74           3\n",
       "4   1693  2.83           2\n",
       "..   ...   ...         ...\n",
       "79  1936  3.71           3\n",
       "80  1810  3.71           1\n",
       "81  1987  3.73           3\n",
       "82  1962  3.76           1\n",
       "83  2050  3.81           2\n",
       "\n",
       "[84 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('csv/1.02. Multiple linear regression.csv')\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we have the same data with one additional variable called, random 1,2,3. That assigns one, two or three randomly to each student and 100% that this variable cannot predict college GPA, so this is our new model.\n",
    "* ![Alt text](img/194.%20Adjusted%20R-Squared3.jpg)\n",
    "    * College GPA is equal to B zero plus B one, times SAT score, plus B two, times the random variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAT</th>\n",
       "      <th>GPA</th>\n",
       "      <th>Rand 1,2,3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>84.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>84.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1845.273810</td>\n",
       "      <td>3.330238</td>\n",
       "      <td>2.059524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>104.530661</td>\n",
       "      <td>0.271617</td>\n",
       "      <td>0.855192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1634.000000</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1772.000000</td>\n",
       "      <td>3.190000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1846.000000</td>\n",
       "      <td>3.380000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1934.000000</td>\n",
       "      <td>3.502500</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2050.000000</td>\n",
       "      <td>3.810000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               SAT        GPA  Rand 1,2,3\n",
       "count    84.000000  84.000000   84.000000\n",
       "mean   1845.273810   3.330238    2.059524\n",
       "std     104.530661   0.271617    0.855192\n",
       "min    1634.000000   2.400000    1.000000\n",
       "25%    1772.000000   3.190000    1.000000\n",
       "50%    1846.000000   3.380000    2.000000\n",
       "75%    1934.000000   3.502500    3.000000\n",
       "max    2050.000000   3.810000    3.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#descriptive statistics\n",
    "data.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create multiple regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Y is GPA.\n",
    "* This time though, we have two explanatory variables, SAT and random 1, 2, 3.\n",
    "* What we can do is declare X one as a data frame containing both series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>GPA</td>       <th>  R-squared:         </th> <td>   0.407</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.392</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   27.76</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 28 Dec 2022</td> <th>  Prob (F-statistic):</th> <td>6.58e-10</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:22:55</td>     <th>  Log-Likelihood:    </th> <td>  12.720</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    84</td>      <th>  AIC:               </th> <td>  -19.44</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    81</td>      <th>  BIC:               </th> <td>  -12.15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>      <td>    0.2960</td> <td>    0.417</td> <td>    0.710</td> <td> 0.480</td> <td>   -0.533</td> <td>    1.125</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>SAT</th>        <td>    0.0017</td> <td>    0.000</td> <td>    7.432</td> <td> 0.000</td> <td>    0.001</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Rand 1,2,3</th> <td>   -0.0083</td> <td>    0.027</td> <td>   -0.304</td> <td> 0.762</td> <td>   -0.062</td> <td>    0.046</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>12.992</td> <th>  Durbin-Watson:     </th> <td>   0.948</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.002</td> <th>  Jarque-Bera (JB):  </th> <td>  16.364</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.731</td> <th>  Prob(JB):          </th> <td>0.000280</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.594</td> <th>  Cond. No.          </th> <td>3.33e+04</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 3.33e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                    GPA   R-squared:                       0.407\n",
       "Model:                            OLS   Adj. R-squared:                  0.392\n",
       "Method:                 Least Squares   F-statistic:                     27.76\n",
       "Date:                Wed, 28 Dec 2022   Prob (F-statistic):           6.58e-10\n",
       "Time:                        21:22:55   Log-Likelihood:                 12.720\n",
       "No. Observations:                  84   AIC:                            -19.44\n",
       "Df Residuals:                      81   BIC:                            -12.15\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.2960      0.417      0.710      0.480      -0.533       1.125\n",
       "SAT            0.0017      0.000      7.432      0.000       0.001       0.002\n",
       "Rand 1,2,3    -0.0083      0.027     -0.304      0.762      -0.062       0.046\n",
       "==============================================================================\n",
       "Omnibus:                       12.992   Durbin-Watson:                   0.948\n",
       "Prob(Omnibus):                  0.002   Jarque-Bera (JB):               16.364\n",
       "Skew:                          -0.731   Prob(JB):                     0.000280\n",
       "Kurtosis:                       4.594   Cond. No.                     3.33e+04\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 3.33e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data['GPA']\n",
    "x1 = data[['SAT', 'Rand 1,2,3']]\n",
    "\n",
    "x = sm.add_constant(x1)\n",
    "results = sm.OLS(y, x).fit()\n",
    "\n",
    "results.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* New vs Old table\n",
    "    * ![Alt text](img/194.%20Adjusted%20R-Squared4.jpg)\n",
    "        * We notice that the new R-squared is 0.407 so it seems as we have increased the explanatory power of the model, but then our enthusiasm is dampened by the adjusted R-squared of 0.392. We were penalized for adding an additional variable that had no strong explanatory power. We have added information but have lost value.\n",
    "            * Adding an impractical variable should be pointed out by the model in some way.\n",
    "        * We have determined a coefficient for the random 1, 2, 3 variable, but its p-value is 0.762. Remember the null hypothesis of the test? Beta two equals zero. **We cannot reject the null hypothesis at the 76% significance level.** This is an incredibly high p-value. For a coefficient to be statistically significant we want a p-value of less than 0.05.\n",
    "        * Our conclusion is :\n",
    "            * that the variable random 1, 2, 3 not only worsens the explanatory power of the model reflected by a lower adjusted R-squared but is also insignificant. \n",
    "            * Therefore, it should be dropped all together. Dropping useless variables is important. \n",
    "                * You can see the original model changed from Y hat equals 0.275 plus 0.0017 times x one. \n",
    "                * Two, Y hat equals 0.296 plus 0.0017 times x one minus 0.0083 times x two.\n",
    "            * The choice of third variable affected the intercept. Whenever you have one variable that is ruining the model you should not use this model altogether because the bias of this variable is reflected into the coefficients of the other variables. \n",
    "                * ![Alt text](img/194.%20Adjusted%20R-Squared5.jpg)\n",
    "            * **The correct approach is to remove it from the regression and run a new one, omitting the problematic predictor.**\n",
    "\n",
    "* The adjusted R-squared is the basis for comparing regression models.\n",
    "    * It only makes sense to compare two models considering the same dependent variable and using the same dataset. \n",
    "        * If we compare two models that are about two different dependent variables we will be making an apples to oranges comparison.\n",
    "        * If we use different data sets it is an apples to dinosaurs problem.\n",
    "            * ![Alt text](img/194.%20Adjusted%20R-Squared6.jpg)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 116: Adjusted R-Squared"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/quiz/4451922#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The adjusted R-squared is a measure that:\n",
    "    * measures how well your model fits the data but penalizes the excessive use of variables\n",
    "    * Like the R-squared, the adjusted R-squared measures how well your model fits the data. However, it penalizes the use of variables that are meaningless for the regression.\n",
    "2. The adjusted R-squared is:\n",
    "    * usually smaller than the R=squared\n",
    "    * Almost always, the adjusted R-squared is smaller than the R-squared. The statement is not true only in the extreme occasions of small sample sizes and a high number of independent variables.\n",
    "3. What can you tell about a new parameter if adding it increases R-squared but decreases adjusted R-squared\n",
    "    * The variable can be omitted since it holds no predictive power"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 195. Multiple Linear Regression Exercise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/10887972#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 196. Test for Significance of the Model (F-Test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/10777052#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Difference Z, T, and F statistics\n",
    "    * ![Alt text](img/196.%20Test%20for%20Significance%20of%20the%20Model%20(F-Test)1.jpg)\n",
    "        * the Z statistic that follows a normal distribution \n",
    "        * the T statistic that follows a student's T distribution\n",
    "        * the F statistic follows an F distribution.\n",
    "* The F statistics test is known as the test for overall significance of the model.\n",
    "    * The null hypothesis is all the betas are equal to zero simultaneously.\n",
    "    * The alternative hypothesis is at least one beta defers from zero.\n",
    "    * ![Alt text](img/196.%20Test%20for%20Significance%20of%20the%20Model%20(F-Test)2.jpg)\n",
    "    * What's the interpretation?\n",
    "        * If all betas are zero, then none of the independent variables matter, therefore our model has no merit.\n",
    "    * ![Alt text](img/196.%20Test%20for%20Significance%20of%20the%20Model%20(F-Test)3.jpg)\n",
    "        * The F statistic is 56.05. The cell below is its P value.\n",
    "        * The cell below is its P value. The number is really low, it is virtually 0.000.\n",
    "            * We say the overall model is significant. The F test is important for regressions as it gives us some important insights.\n",
    "    * Let's see the table for the model where we added the random 1, 2, 3 variable that had nothing to do with anything.\n",
    "        * ![Alt text](img/196.%20Test%20for%20Significance%20of%20the%20Model%20(F-Test)4.jpg)\n",
    "        * The F statistic is 27.76, and the P value is 0.000. \n",
    "            * You can see the F statistic is lower. The model is still significant but less so.\n",
    "        * The lower the F statistic, the closer to a non-significant model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 197. OLS Assumptions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/10777056#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Regression assumptions\n",
    "* ![Alt text](img/197.%20OLS%20Assumptions1.jpg)\n",
    "    * Linearity or Linear Regression\n",
    "        * Each independent variable is multiplied by a coefficient and summed up to predict the value.\n",
    "    * No Endogeneity of Regressors\n",
    "        * Mathematically, this is expressed as the covariance of the error, and the Xs is 0 for any error or X.\n",
    "    * Normality and homoscedasticity\n",
    "        * The expected value of the error is zero as we expect to have no errors on average.\n",
    "        * Homoscedasticity in plain English means **constant variance.**\n",
    "    * No autocorrelation\n",
    "        * Mathematically, the covariance of any two error terms is zero. \n",
    "        * That's the assumption that would usually stop you from using a linear regression in your analysis.\n",
    "        * No Multicollinearity\n",
    "            * Multicollinearity is observed when two or more variables have a high correlation between each other.\n",
    "\n",
    "* **At your workplace, the biggest mistake you can make is to perform a regression that violates one of these assumptions.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 117: OLS Assumptions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/quiz/4451924#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. If a regression assumption is violated:\n",
    "    * Performing regression analysis will yield an incorrect result.\n",
    "    * Nothing stops you from performing the regression analysis, but it will yield an incorrect result. It is a big deal."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 198. A1: Linearity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/10777058#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A linear regression is the simplest non-trivial relationship.\n",
    "    * Each independent variable is multiplied by a coefficient, and summed up to predict the value of the dependent variable.\n",
    "* How can you verify if the relationship between two variables is linear?\n",
    "    * The easiest way is to choose an independent variable x1, and plot it against the dependent y on a scatter plot.\n",
    "    * If the data points form a pattern that looks like a straight line, then a linear regression model is suitable.\n",
    "    * ![Alt text](img/198.%20A1%20Linearity1.jpg)\n",
    "\n",
    "* Case where the assumption is violated:\n",
    "    * After plotting another variable x2 against y on a scatter plot, we see there is no straight line that fits the data well. Actually, a curved line would be a very good fit. **Using a linear regression would not be appropriate.**\n",
    "        * ![Alt text](img/198.%20A1%20Linearity2.jpg)\n",
    "    * How to fixes:\n",
    "    * ![Alt text](img/198.%20A1%20Linearity3.jpg)\n",
    "        * Run a non-linear regression, as we will do in the next section, or transform your relationship.\n",
    "        * There are exponential and \n",
    "        * logarithmical transformations that help with that.\n",
    "    * The quadratic relationship we saw in this video could be easily transformed into a straight line with the appropriate methods. The takeaway is, if the relationship is non-linear, you should not use the data before transforming it appropriately."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 118: A1: Linearity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/quiz/4451926#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Linearity is easy to relax.\n",
    "    * True\n",
    "2. What should you do if you want to employ a linear regression but the relationship in your data is not linear?\n",
    "    * Transform it appropriately before using it"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 199. A2: No Endogeneity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/10777086#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* No Endogeneity of Regressors is refers to the prohibition of a link between the independent variables and the errors, mathematically expressed in the following way.\n",
    "    * ![Alt text](img/199.%20A2%20No%20Endogeneity1.jpg)\n",
    "    * The error, thus the difference between the observed values and the predicted values, is correlated with our independent values.\n",
    "    * **This is a problem referred to as omitted variable bias.**\n",
    "* Omitted variable bias is introduced to the model when you forget to include a relevant variable.\n",
    "    * As each independent variable explains y, they move together and are somewhat correlated.\n",
    "    * Similarly, y is also explained by the omitted variable, so they are also correlated.\n",
    "    * Chances are, the omitted variable is also correlated with at least one independent x.\n",
    "    * However, you forgot to include it as a regressor. Everything that you don't explain with your model goes into the error. Because of that, the error becomes correlated with everything else.\n",
    "        * ![Alt text](img/199.%20A2%20No%20Endogeneity2.jpg)\n",
    "\n",
    "* Omitted variable bias is hard to fix.\n",
    "    * ![Alt text](img/199.%20A2%20No%20Endogeneity3.jpg)\n",
    "    * We have only one variable, but when your model is exhaustive with 10 variables or more, you may feel disheartened.\n",
    "\n",
    "* Example:\n",
    "    * Why is bigger real estate cheaper?\n",
    "        * ![Alt text](img/199.%20A2%20No%20Endogeneity4.jpg)\n",
    "        * The sample comprises apartment buildings in central London, and is large. So the problem is not with the sample.\n",
    "        * What is it about smaller size that is making it so expensive? \n",
    "        * Where are the small houses? \n",
    "        * There is rarely construction of new apartment buildings in central London. And then you realize the City of London was in the sample. The place where most buildings are skyscrapers, with some of the most valuable real estate in the world. We omitted the exact location as a variable.\n",
    "        * After we included a variable that measures if the property is in London's City, everything falls into place. Larger properties are more expensive, and vice versa.\n",
    "    * The incorrect exclusion of a variable, like in this case, leads to biased and counterintuitive estimates that are toxic to your regression analysis.\n",
    "\n",
    "* Omitted variable bias is complex.\n",
    "    * It is always different, \n",
    "    * always sneaky, and \n",
    "    * only experience and advanced knowledge on the subject can help. \n",
    "    * Always check for it, and if you can't think of anything, ask a colleague for assistance.\n",
    "        * ![Alt text](img/199.%20A2%20No%20Endogeneity5.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 119: A2: No Endogeneity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/quiz/4451928#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The easiest way to detect an omitted variable bias is through:\n",
    "    * the error term\n",
    "    * Omitted variable bias occurs when you forget to include a variable. This is reflected in the error term as the factor you forgot about is included in the error. In this way, the error is not random but includes a systematic part (the omitted variable)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 200. A3: Normality and Homoscedasticity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/10777112#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ![Alt text](img/200.%20A3%20Normality%20and%20Homoscedasticity1.jpg)\n",
    "    * Normality\n",
    "        * ![Alt text](img/200.%20A3%20Normality%20and%20Homoscedasticity2.jpg)\n",
    "        * We assume the error term is normally distributed. Normal distribution is not required for creating the regression, but for making inferences.\n",
    "        * What should we do, if the error term, is not normally distributed?\n",
    "            * The central limit theorem. For large samples, the central limit theorem applies for the error terms too. Therefore, we can consider normality as a given for us. Therefore, we can consider normality as a given for us.\n",
    "    * zero mean\n",
    "        * ![Alt text](img/200.%20A3%20Normality%20and%20Homoscedasticity3.jpg)\n",
    "        * if the mean is not expected to be zero, then the line is not the best fitting one.\n",
    "        * However, having an intercept solves that problem. So in real life, it is unusual to violate this part of the assumption.\n",
    "    * homoscedasticity of the error term.\n",
    "        * ![Alt text](img/200.%20A3%20Normality%20and%20Homoscedasticity4.jpg)\n",
    "        * It means to have equal variance.\n",
    "        * So, the error term should have equal variance, one with the other.\n",
    "        * What if there was a pattern in the variance?\n",
    "            * an example of a data set, where errors have a different variance (heteroscedastic). Starting close to the regression line, and going further away. This would imply that, for smaller values, of the independent and dependent variables, we would have a better prediction than for bigger values. And, I assure you, we really don't like this uncertainty.\n",
    "* Prevention:3\n",
    "    * ![Alt text](img/200.%20A3%20Normality%20and%20Homoscedasticity5.jpg)\n",
    "    * First, check for omitted variable bias.\n",
    "    * After that, you can look for outliers, and try to remove them.\n",
    "    * the log transformation. Log stand for logarithm\n",
    "        * ![Alt text](img/200.%20A3%20Normality%20and%20Homoscedasticity6.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Example:\n",
    "    * ![Alt text](img/200.%20A3%20Normality%20and%20Homoscedasticity7.jpg)\n",
    "    * ![Alt text](img/200.%20A3%20Normality%20and%20Homoscedasticity8.jpg)\n",
    "        * On the left hand side of the chart, the variance of the error is small.\n",
    "        * While on the right, it is high.\n",
    "            * Here's the model. \n",
    "            * As X increases by one unit, Y grows, by b1 units.\n",
    "    * Transforming the X variable to a new variable, called log of x, and plot the data. Changing the scale of X would reduce the width of the graph.\n",
    "        * ![Alt text](img/200.%20A3%20Normality%20and%20Homoscedasticity9a.jpg)\n",
    "        * ![Alt text](img/200.%20A3%20Normality%20and%20Homoscedasticity9b.jpg)\n",
    "    * What if we transform the Y scale instead? Autologically, to what happened previously, we would expect the height of the graph to be reduced. This new model is also called semi-log model. It's meaning is, as X increases by one unit, Y changes, by b1 percent.\n",
    "        * ![Alt text](img/200.%20A3%20Normality%20and%20Homoscedasticity10a.jpg)\n",
    "        * ![Alt text](img/200.%20A3%20Normality%20and%20Homoscedasticity10b.jpg)\n",
    "    * Sometimes, we want or need to change, both scales to log. The result is a log-log model. We shrink the graph in height, and in width. The improvement is noticeable, but not game changing. However, we may be sure the assumption is not violated. The interpretation is, for each percentage point change in x, Y changes by b1 percentage points. If you've done economics, you would recognize such a relationship, is known as elasticity.\n",
    "        * ![Alt text](img/200.%20A3%20Normality%20and%20Homoscedasticity11a.jpg)\n",
    "        * ![Alt text](img/200.%20A3%20Normality%20and%20Homoscedasticity11b.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 201. A4: No Autocorrelation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/10777114#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* No Autocorrelation or no serial correlation\n",
    "    * Errors are assumed to be uncorrelated.\n",
    "    * Where can we observe serial correlation between errors? \n",
    "        * It is highly unlikely to find it in data taken at one moment of time, known as cross-sectional data. Very common in time series data.\n",
    "        * ![Alt text](img/201.%20A4%20No%20Autocorrelation1.jpg)\n",
    "* example: stock prices.\n",
    "    * ![Alt text](img/201.%20A4%20No%20Autocorrelation2.jpg)\n",
    "        * Every day you have a new quote for the same stock. These new numbers you see have the same underlying asset. \n",
    "        * Without getting too much into the finance, ideally you want them to be random or predicted by macro factors such as GDP, tax rate, political events and so on.\n",
    "        * Unfortunately, it is common in underdeveloped markets to see patterns in the stock prices.\n",
    "        * There is a well known phenomenon called The Day-of-the-Week Effect.\n",
    "    * The Day-of-the-Week Effect consists in disproportionately high returns on Fridays and low returns on Mondays. There is no consensus on the true nature of the day-of-the-week effect.\n",
    "        * One possible explanation proposed by Noble Prize winner Merton Miller is that investors don't have time to read all the news immediately.\n",
    "            * ![Alt text](img/201.%20A4%20No%20Autocorrelation3.jpg)\n",
    "            * So they do it over the weekend. \n",
    "                * The first day to respond to negative information is on Mondays.\n",
    "                * Then during the week, their advisors give them new positive information and they start buying on Thursdays and Fridays.\n",
    "        * Another famous explanation is given by the distinguished financier, Kenneth French, who suggested firms delay bad news for the weekends so markets react on Mondays.\n",
    "            * ![Alt text](img/201.%20A4%20No%20Autocorrelation4.jpg)\n",
    "        * Whatever the reason, there is correlation of the errors when building regressions about stock prices.\n",
    "            * The first observation, the sixth, the 11th and every fifth onwards would be Mondays.\n",
    "            * The fifth, 10th and so on would be Fridays. \n",
    "            * Errors on Mondays would be biased downwards and errors for Fridays would be biased upwards.\n",
    "            * The mathematics of the linear regression does not consider this. It assumes errors should be randomly spread around the regression line.\n",
    "\n",
    "* So how does one detect auto correlation?\n",
    "    * A common way is to plot all the residuals on a graph and look for patterns.\n",
    "        * ![Alt text](img/201.%20A4%20No%20Autocorrelation5.jpg)\n",
    "    * Another is the Durbin Watson test, which you have in the summary for the table provided by stats models.\n",
    "        * Generally, its values fall between zero and four. Two indicates no auto correlation, while values below one and above three are a cause for alarm.\n",
    "        * ![Alt text](img/201.%20A4%20No%20Autocorrelation6.jpg)\n",
    "\n",
    "* The only thing you can do is avoid using a linear regression in such a setting.\n",
    "    * There are other types of regressions that deal with time series data.\n",
    "        * auto aggress model,\n",
    "        * moving average model, \n",
    "        * auto regressive moving average model,\n",
    "        * auto regressive integrated moving average model.\n",
    "        * ![Alt text](img/201.%20A4%20No%20Autocorrelation7.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 120: A4: No autocorrelation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/quiz/4451932#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Autocorrelation is not likely to be observed in:\n",
    "    * cross-sectional data\n",
    "    * Autocorrelation is not observed in cross-sectional data. You usually spot it at time series data, which is a subset of panel data. Sample data is not relevant for this question.\n",
    "2. How do you fix autocorrelation when using a linear regression model?\n",
    "    * "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 202. A5: No Multicollinearity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/10777116#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* No Multicollinearity\n",
    "    * We observe multicollinearity when two or more variables have a high correlation.\n",
    "        * ![Alt text](img/202.%20A5%20No%20Multicollinearity1.jpg)\n",
    "            * A is equal to two plus five times B. A and B are two variables with an exact linear combination. \n",
    "            * A can be represented using B and B can be represented using A. \n",
    "            * In a model containing A and B, we would have perfect multicollinearity. \n",
    "            * This imposes a big problem to our regression model as the coefficients will be wrongly estimated.\n",
    "            * The reasoning is that, if A can be represented using B, there is no point using both.\n",
    "        * ![Alt text](img/202.%20A5%20No%20Multicollinearity2.jpg)\n",
    "            * Another example would be two variables, C and D, with a correlation of 90%.\n",
    "            * If we had a regression model using C and D, we would also have multicollinearity, although not perfect. \n",
    "            * Here, the assumption is still violated and poses a problem to our model.\n",
    "\n",
    "* How to fixes\n",
    "    * ![Alt text](img/202.%20A5%20No%20Multicollinearity3.jpg)\n",
    "        * The first one is to drop one of the two variables.\n",
    "        * The second is to transform them into one variable.\n",
    "        * keep them both while treating them with extreme caution.\n",
    "    * The correct approach depends on the research at hand.\n",
    "    * Multicollinearity is a big problem, but is also the easiest to notice.\n",
    "\n",
    "* Prevention:\n",
    "    * ![Alt text](img/202.%20A5%20No%20Multicollinearity4.jpg)\n",
    "    * Before creating the regression, find the correlation between each two pairs of independent variables and you will know if a multicollinearity problem may arise.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 121: A5: No Multicollinearity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/quiz/4451934#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. No multicollinearity is:\n",
    "    * easy to spot and easy to fix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 203. Dealing with Categorical Data - Dummy Variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/10777118#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In regression analysis, a dummy is a variable that is used to include categorical data into a regression model another meaning, an invitation or a copy that stands as a substitute.\n",
    "* Imitating the categories with numbers.\n",
    "    * Example:\n",
    "        * Gender:\n",
    "            * Male -> 0\n",
    "            * Female -> 1\n",
    "        * Attendance:\n",
    "            * No -> 0\n",
    "            * Yes -> 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels as sts\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAT</th>\n",
       "      <th>GPA</th>\n",
       "      <th>Attendance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1714</td>\n",
       "      <td>2.40</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1664</td>\n",
       "      <td>2.52</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1760</td>\n",
       "      <td>2.54</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1685</td>\n",
       "      <td>2.74</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1693</td>\n",
       "      <td>2.83</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1936</td>\n",
       "      <td>3.71</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1810</td>\n",
       "      <td>3.71</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1987</td>\n",
       "      <td>3.73</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1962</td>\n",
       "      <td>3.76</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>2050</td>\n",
       "      <td>3.81</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     SAT   GPA Attendance\n",
       "0   1714  2.40         No\n",
       "1   1664  2.52         No\n",
       "2   1760  2.54         No\n",
       "3   1685  2.74         No\n",
       "4   1693  2.83         No\n",
       "..   ...   ...        ...\n",
       "79  1936  3.71        Yes\n",
       "80  1810  3.71        Yes\n",
       "81  1987  3.73         No\n",
       "82  1962  3.76        Yes\n",
       "83  2050  3.81        Yes\n",
       "\n",
       "[84 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv('csv/1.03. Dummies.csv')\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAT</th>\n",
       "      <th>GPA</th>\n",
       "      <th>Attendance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1714</td>\n",
       "      <td>2.40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1664</td>\n",
       "      <td>2.52</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1760</td>\n",
       "      <td>2.54</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1685</td>\n",
       "      <td>2.74</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1693</td>\n",
       "      <td>2.83</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1936</td>\n",
       "      <td>3.71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1810</td>\n",
       "      <td>3.71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1987</td>\n",
       "      <td>3.73</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1962</td>\n",
       "      <td>3.76</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>2050</td>\n",
       "      <td>3.81</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     SAT   GPA  Attendance\n",
       "0   1714  2.40           0\n",
       "1   1664  2.52           0\n",
       "2   1760  2.54           0\n",
       "3   1685  2.74           0\n",
       "4   1693  2.83           0\n",
       "..   ...   ...         ...\n",
       "79  1936  3.71           1\n",
       "80  1810  3.71           1\n",
       "81  1987  3.73           0\n",
       "82  1962  3.76           1\n",
       "83  2050  3.81           1\n",
       "\n",
       "[84 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = raw_data.copy()\n",
    "\n",
    "data['Attendance'] = data['Attendance'].map({'Yes':1, 'No':0})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAT</th>\n",
       "      <th>GPA</th>\n",
       "      <th>Attendance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>84.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>84.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1845.273810</td>\n",
       "      <td>3.330238</td>\n",
       "      <td>0.464286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>104.530661</td>\n",
       "      <td>0.271617</td>\n",
       "      <td>0.501718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1634.000000</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1772.000000</td>\n",
       "      <td>3.190000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1846.000000</td>\n",
       "      <td>3.380000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1934.000000</td>\n",
       "      <td>3.502500</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2050.000000</td>\n",
       "      <td>3.810000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               SAT        GPA  Attendance\n",
       "count    84.000000  84.000000   84.000000\n",
       "mean   1845.273810   3.330238    0.464286\n",
       "std     104.530661   0.271617    0.501718\n",
       "min    1634.000000   2.400000    0.000000\n",
       "25%    1772.000000   3.190000    0.000000\n",
       "50%    1846.000000   3.380000    0.000000\n",
       "75%    1934.000000   3.502500    1.000000\n",
       "max    2050.000000   3.810000    1.000000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ![Alt text](img/203.%20Dealing%20with%20Categorical%20Data%20-%20Dummy%20Variables1.jpg)\n",
    "    * Notice that the mean of attended more than 75% is 0.46. The fact that the mean is less than 0.5, gives us the information that there are more zeros than ones.\n",
    "    * Since the two outcomes are zero and one, this implies that 46% of the students have attended more than 75% of the lessons in any case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>GPA</td>       <th>  R-squared:         </th> <td>   0.565</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.555</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   52.70</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 28 Dec 2022</td> <th>  Prob (F-statistic):</th> <td>2.19e-15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:22:57</td>     <th>  Log-Likelihood:    </th> <td>  25.798</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    84</td>      <th>  AIC:               </th> <td>  -45.60</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    81</td>      <th>  BIC:               </th> <td>  -38.30</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>      <td>    0.6439</td> <td>    0.358</td> <td>    1.797</td> <td> 0.076</td> <td>   -0.069</td> <td>    1.357</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>SAT</th>        <td>    0.0014</td> <td>    0.000</td> <td>    7.141</td> <td> 0.000</td> <td>    0.001</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attendance</th> <td>    0.2226</td> <td>    0.041</td> <td>    5.451</td> <td> 0.000</td> <td>    0.141</td> <td>    0.304</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>19.560</td> <th>  Durbin-Watson:     </th> <td>   1.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  27.189</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-1.028</td> <th>  Prob(JB):          </th> <td>1.25e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.881</td> <th>  Cond. No.          </th> <td>3.35e+04</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 3.35e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                    GPA   R-squared:                       0.565\n",
       "Model:                            OLS   Adj. R-squared:                  0.555\n",
       "Method:                 Least Squares   F-statistic:                     52.70\n",
       "Date:                Wed, 28 Dec 2022   Prob (F-statistic):           2.19e-15\n",
       "Time:                        21:22:57   Log-Likelihood:                 25.798\n",
       "No. Observations:                  84   AIC:                            -45.60\n",
       "Df Residuals:                      81   BIC:                            -38.30\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.6439      0.358      1.797      0.076      -0.069       1.357\n",
       "SAT            0.0014      0.000      7.141      0.000       0.001       0.002\n",
       "Attendance     0.2226      0.041      5.451      0.000       0.141       0.304\n",
       "==============================================================================\n",
       "Omnibus:                       19.560   Durbin-Watson:                   1.009\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               27.189\n",
       "Skew:                          -1.028   Prob(JB):                     1.25e-06\n",
       "Kurtosis:                       4.881   Cond. No.                     3.35e+04\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 3.35e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data['GPA']\n",
    "x1 = data[['SAT', 'Attendance']]\n",
    "\n",
    "x = sm.add_constant(x1)\n",
    "results = sm.OLS(y, x).fit()\n",
    "results.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The result is the following.\n",
    "    * Our overall model is significant.\n",
    "        * ![Alt text](img/203.%20Dealing%20with%20Categorical%20Data%20-%20Dummy%20Variables2.jpg)\n",
    "    * The SAT score is significant and the dummy is significant.\n",
    "        * ![Alt text](img/203.%20Dealing%20with%20Categorical%20Data%20-%20Dummy%20Variables3.jpg)\n",
    "    * The adjusted R squared of this model is 0.555.\n",
    "        * ![Alt text](img/203.%20Dealing%20with%20Categorical%20Data%20-%20Dummy%20Variables4.jpg)\n",
    "\n",
    "* Which is a great improvement from what we got without attendance.\n",
    "    * ![Alt text](img/203.%20Dealing%20with%20Categorical%20Data%20-%20Dummy%20Variables5.jpg)\n",
    "* ![Alt text](img/203.%20Dealing%20with%20Categorical%20Data%20-%20Dummy%20Variables6.jpg)\n",
    "    * The original model without the dummy variable was, GPA equals 0.275, plus 0.0017 times the SAT score of a student. \n",
    "    * The model, including the dummy variable, is GPA equals 0.6439, plus 0.0014 times the SAT score of a student plus 0.226 times the dummy.\n",
    "    * We can represent this equation with two others:\n",
    "        * ![Alt text](img/203.%20Dealing%20with%20Categorical%20Data%20-%20Dummy%20Variables7.jpg)\n",
    "            * If the student did not attend, the dummy will be zero. So 0.2226 times zero is zero. The model becomes, GPA equals 0.6439 plus 0.0014 times SAT.\n",
    "            * If the student attended, the dummy will be one. So the model becomes GPA equals 0.6439 plus 0.0014 times SAT plus 0.2226. Let's add the intercept and the dummy together. We get GPA equals 0.8665 plus 0.0014 times SAT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# regression dummies\n",
    "\n",
    "plt.scatter(data['SAT'], y)\n",
    "yhat_no = 0.6439 + 0.0014*data['SAT']\n",
    "yhat_yes = 0.8665 + 0.0014*data['SAT']\n",
    "fig = plt.plot(data['SAT'], yhat_no, lw=2, c='red')\n",
    "fig = plt.plot(data['SAT'], yhat_yes, lw=2, c='blue')\n",
    "plt.xlabel('SAT', fontsize=20)\n",
    "plt.ylabel('GPA', fontsize=20)\n",
    "plt.show()\n",
    "\n",
    "# blue line = student attendance yes\n",
    "# red line = student attendance no"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* So what we observe are two equations; that have the same slope, but have a different intercept.\n",
    "    * The students who attended are spread around the upper line. On average, their GPA (dummy coef) is 0.2226 higher than the GPA of students who did not attend.\n",
    "        * ![Alt text](img/203.%20Dealing%20with%20Categorical%20Data%20-%20Dummy%20Variables8.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# regression dummies + data color\n",
    "\n",
    "plt.scatter(data['SAT'], y, c=data['Attendance'], cmap='bwr_r')\n",
    "yhat_no = 0.6439 + 0.0014*data['SAT']\n",
    "yhat_yes = 0.8665 + 0.0014*data['SAT']\n",
    "fig = plt.plot(data['SAT'], yhat_no, lw=2, c='red')\n",
    "fig = plt.plot(data['SAT'], yhat_yes, lw=2, c='blue')\n",
    "plt.xlabel('SAT', fontsize=20)\n",
    "plt.ylabel('GPA', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# original + dummies\n",
    "\n",
    "plt.scatter(data['SAT'], y, c=data['Attendance'], cmap='bwr_r')\n",
    "yhat_no = 0.6439 + 0.0014*data['SAT']\n",
    "yhat_yes = 0.8665 + 0.0014*data['SAT']\n",
    "yhat = 0.0017*data['SAT'] + 0.275\n",
    "fig = plt.plot(data['SAT'], yhat_no, lw=2, c='red', label='regression line1')\n",
    "fig = plt.plot(data['SAT'], yhat_yes, lw=2, c='blue', label='regression line2')\n",
    "fig = plt.plot(data['SAT'], yhat, lw=3, c='orange', label='regression line')\n",
    "plt.xlabel('SAT', fontsize=20)\n",
    "plt.ylabel('GPA', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ![Alt text](img/203.%20Dealing%20with%20Categorical%20Data%20-%20Dummy%20Variables9.jpg)\n",
    "    * To use this model for prediction purposes, we need two pieces of information, an SAT score, and whether a person attended more than 75% of their lectures."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 204. Dealing with Categorical Data - Dummy Variables Exercise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/10888002#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 205. Making Predictions with the Linear Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/10777122#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There's a stats model method which takes a data frame organized in a similar way to the one we use to fit the model and then makes predictions.\n",
    "    * We worked with a variable X for that regression."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to make predictions based on the regression we create on '203. Dealing with Categorical Data - Dummy Variables'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 'x' is a data frame with three columns: a constant, SAT, and attendance. This constant was actually added with the add constant method we used prior to fitting the model. \n",
    "    * x = sm.add_constant(x1)\n",
    "* It is a simulation of x zero and contains only ones.\n",
    "    * yhat = b0 + b1x1\n",
    "    * yhat= b0 * 1 + b1x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>const</th>\n",
       "      <th>SAT</th>\n",
       "      <th>Attendance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1714</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1664</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1760</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1685</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1693</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1936</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1810</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1962</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2050</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    const   SAT  Attendance\n",
       "0     1.0  1714           0\n",
       "1     1.0  1664           0\n",
       "2     1.0  1760           0\n",
       "3     1.0  1685           0\n",
       "4     1.0  1693           0\n",
       "..    ...   ...         ...\n",
       "79    1.0  1936           1\n",
       "80    1.0  1810           1\n",
       "81    1.0  1987           0\n",
       "82    1.0  1962           1\n",
       "83    1.0  2050           1\n",
       "\n",
       "[84 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's create a data frame which we will use for predictions.\n",
    "    * We will organize it in the same way.\n",
    "    * It will have three columns: constant, SAT, and attendance.\n",
    "* We will check two students:\n",
    "    * ![Alt text](img/205.%20Making%20Predictions%20with%20the%20Linear%20Regression1.jpg)\n",
    "    * Bob, who got 1700 on the SAT and did not attend 75% of the lessons, and Alice who got 1670 on the SAT and attended more than 75% of the lectures.\n",
    "\n",
    "* STEP BY STEP\n",
    "    1. The new variable will be called new data. We want it to be a pandas data frame.\n",
    "        * The first column is called Const and is always one. \n",
    "        * The second column is SAT and has two entries, 1700 and 1670.\n",
    "        * The third column is called attendance and has two entries, zero and one. \n",
    "    2. Since data frames usually arrange columns in alphabetical order, I'll overwrite new data with new data but the columns will be in the order specified by me.\n",
    "    3. The appropriate method that allows us to predict the values is the fitted regression dot predict.\n",
    "        * The fitted regression for us is in the variable results.\n",
    "        * This method has a single argument, the new data. Therefore, I'll create a variable called predictions which will contain our predicted values.\n",
    "    4. Next is transform it into a data frame and join it with the first one.\n",
    "        * The predicted GPA at graduation for Bob is 3.02, while for Alice 3.20. \n",
    "        * Alice scored lower on her SAT but she attended more than 75% of her lectures and she is predicted to graduate with a significantly higher GPA than Bob.\n",
    "        * Had we used our previous model without the dummy variable we would've predicted that Bob would graduate with 3.17 and Alice with 3.12. We can easily see the difference in why exactly we needed the dummy variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>const</th>\n",
       "      <th>SAT</th>\n",
       "      <th>Attendance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1670</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   const   SAT  Attendance\n",
       "0      1  1700           0\n",
       "1      1  1670           1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Filtering only SAT 1700 and 1670\n",
    "\n",
    "new_data = pd.DataFrame({'const':1, 'SAT':[1700, 1670], 'Attendance':[0,1]})\n",
    "new_data = new_data[['const', 'SAT', 'Attendance']]\n",
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>const</th>\n",
       "      <th>SAT</th>\n",
       "      <th>Attendance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bob</th>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alice</th>\n",
       "      <td>1</td>\n",
       "      <td>1670</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       const   SAT  Attendance\n",
       "Bob        1  1700           0\n",
       "Alice      1  1670           1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Transform 0 to 'Bob' and 1 to 'Alice'\n",
    "\n",
    "new_data.rename(index={0:'Bob', 1:'Alice'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3.023513\n",
       "1    3.204163\n",
       "dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. fitted regression is in the variable results -> results = sm.OLS(y, x).fit()\n",
    "\n",
    "predictions = results.predict(new_data)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>const</th>\n",
       "      <th>SAT</th>\n",
       "      <th>Attendance</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bob</th>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>0</td>\n",
       "      <td>3.023513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alice</th>\n",
       "      <td>1</td>\n",
       "      <td>1670</td>\n",
       "      <td>1</td>\n",
       "      <td>3.204163</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       const   SAT  Attendance  predictions\n",
       "Bob        1  1700           0     3.023513\n",
       "Alice      1  1670           1     3.204163"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Transform it into data frame and join with the first one\n",
    "\n",
    "predictions_df = pd.DataFrame({'predictions':predictions})\n",
    "joined = new_data.join(predictions_df)\n",
    "joined.rename(index={0:'Bob', 1:'Alice'})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Section 34: Advanced Statistical Methods - Linear Regression with sklearn**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 206. What is sklearn and How is it Different from Other Packages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/14390298#content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Scikit-learn, now better known as SKLearn, is a machine learning package for Python.\n",
    "* It's name arguably comes from SciPy Toolkit, meaning in addition on top of the popular package SciPy. \n",
    "* In fact, SKLearn is built on NumPy, SciPy, and matplotlib.\n",
    "* ![Alt text](img/206.%20What%20is%20sklearn%20and%20How%20is%20it%20Different%20from%20Other%20Packages1.jpg)\n",
    "    * SKLearn is very fast and efficient,\n",
    "    * it often prefers working with arrays.\n",
    "* ![Alt text](img/206.%20What%20is%20sklearn%20and%20How%20is%20it%20Different%20from%20Other%20Packages2.jpg)\n",
    "    * So far, we've worked mainly with Panda's data frames because of stats models. \n",
    "    * Now, however, even if we pre-process data in Pandas, we may need to transform it into an ndarray before feeding it to the algorithm.\n",
    "* Major advantages of SKLearn\n",
    "    * ![Alt text](img/206.%20What%20is%20sklearn%20and%20How%20is%20it%20Different%20from%20Other%20Packages3.jpg)\n",
    "    * First, it boasts incredible documentation. Whatever doubts you may have about how it works, SKLearn has it on its website, usually with example applications.\n",
    "    * Second, its variety. In terms of machine learning, SKLearn is definitely the leading package right now. Regression, classification, clustering, support vector machines, dimensionality reduction, everything is there. \n",
    "        * The only weak spot in its range is deep learning. TensorFlow, Keras, and PyTorch are much better alternatives in that case.\n",
    "    * Finally, SKLearn is famously numerically stable. The basic idea is that training an algorithm is about performing complicated mathematical operations in the background. When the numbers you are dealing with are too small or too big, your code may break, not because of your programming skills but because the library you are using has some minor inefficiencies, which may turn into big problems. SKLearn rarely has those, and the community actively contributes to the package whenever an issue arises."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 207. How are we Going to Approach this Section?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/14390320#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The advanced statistical methods we are covering in these lectures linear and logistic regressions, often fall under the term machine learning according to numerous sources before was traditional statistical methods (manual calculation), and you may feel free to refer to them as machine learning (ML).\n",
    "    * ![Alt text](img/207.%20How%20are%20we%20Going%20to%20Approach%20this%20Section1.jpg)\n",
    "\n",
    "* Statsmodels is a great library for learning purposes, but in practice **sklearn** is the preferred choice of most professionals.\n",
    "    * ![Alt text](img/207.%20How%20are%20we%20Going%20to%20Approach%20this%20Section2.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 208. Simple Linear Regression with sklearn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/14390322#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAT</th>\n",
       "      <th>GPA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1714</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1664</td>\n",
       "      <td>2.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1760</td>\n",
       "      <td>2.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1685</td>\n",
       "      <td>2.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1693</td>\n",
       "      <td>2.83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SAT   GPA\n",
       "0  1714  2.40\n",
       "1  1664  2.52\n",
       "2  1760  2.54\n",
       "3  1685  2.74\n",
       "4  1693  2.83"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('csv/1.01. Simple linear regression.csv')\n",
    "\n",
    "# summary the first 5 column\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Regression and Declare the dependent and Independent variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Predicting college GPA using single feature, the SAT score.\n",
    "        * Since the word variable has several meanings, on machine learning we use the word feature instead.\n",
    "\n",
    "* There are three types of machine learning:\n",
    "    * supervised, \n",
    "    * unsupervised, and \n",
    "    * reinforcement.\n",
    "\n",
    "* For now, it is important to know that we've only seen supervised learning. In supervised learning, we've got inputs and targets. \n",
    "    * ![Alt text](img/208.%20Simple%20Linear%20Regression%20with%20sklearn1.jpg)\n",
    "        * The inputs are the features we use to predict an outcome, in our case, the SAT score.\n",
    "        * The targets are the correct values we are aiming for, in our case, the GPA.\n",
    "        * Usually the targets are historical values that correspond to a given SAT score. \n",
    "        * In fact, our machine learning algorithm will find the optimal coefficients of a linear regression model, that when given an SAT score, best predict the GPA of a student."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature/input\n",
    "x = data['SAT'] \n",
    "\n",
    "#target/output\n",
    "y = data['GPA'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking shapes input\n",
    "x.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking shapes target\n",
    "y.shape "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* They are both vectors of length 84.\n",
    "* We can proceed to feeding them to sklearn algorithm. Sklearn takes full advantage of the object-oriented capabilities of Python and works a lot with very well written classes. More often than not, we would need to create an object or an instance of the given class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After executing this line, reg, will be an instance of the linear regression class.\n",
    "reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afterwards, all we need to do is fit the regression.\n",
    "# Order is very important because sklearn has a different order than stats models.\n",
    "# x = input\n",
    "# y = target\n",
    "\n",
    "# reg.fit(x, y) <- this code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Above Code will got error because 'x' is single array and SKlearn expected it for 2D array, but got 1D array instead.\n",
    "* Basically, our inputs are a one dimensional object, which sklearn does not fancy.\n",
    "* What we must do is reshape it into a matrix. Recall that that's achieved through the reshape method.\n",
    "    * ![Alt text](img/208.%20Simple%20Linear%20Regression%20with%20sklearn2.jpg)\n",
    "\n",
    "* Note that this is an issue only for a simple linear regression. If we had more than one feature, we would not have this complication. The assumption the developers had is that normally, sklearn expects dozens of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84, 1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_matrix = x.values.reshape(-1,1)\n",
    "x_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rerun the code again with x_matrix\n",
    "\n",
    "reg.fit(x_matrix, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'copy_X': True, 'fit_intercept': True, 'n_jobs': None, 'positive': False}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new version of the sklearn library not showing parameter value anymore, \n",
    "# so we add .get_params() method to check all parameter value\n",
    "reg.fit(x_matrix, y).get_params()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 209. Simple Linear Regression with sklearn - A StatsModels-like Summary Table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/14390338#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* All of the parameters you see take the default value set by the module.\n",
    "    * New version of scikit-learn the output is:\n",
    "        * ![Alt text](img/209.%20Simple%20Linear%20Regression%20with%20sklearn%20-%20A%20StatsModels-like%20Summary%20Table0b.jpg)\n",
    "        \n",
    "    * This lecture using scikit-learn older version so the output is :\n",
    "        * ![Alt text](img/209.%20Simple%20Linear%20Regression%20with%20sklearn%20-%20A%20StatsModels-like%20Summary%20Table0a.jpg)\n",
    "    \n",
    "        \n",
    "    * normalize\n",
    "        * ![Alt text](img/209.%20Simple%20Linear%20Regression%20with%20sklearn%20-%20A%20StatsModels-like%20Summary%20Table1.jpg)\n",
    "            * Standardization was the process of subtracting the mean and dividing by the standard deviation.\n",
    "            * normalization means we subtract the mean.\n",
    "    * Copy_X\n",
    "        * When set to True, it copies the inputs before fitting them.\n",
    "        * This is a safety net against normalization and other transformations that can be done by SK Learn while creating an algorithm.\n",
    "    * fit_intercept\n",
    "        * ![Alt text](img/209.%20Simple%20Linear%20Regression%20with%20sklearn%20-%20A%20StatsModels-like%20Summary%20Table2.jpg)\n",
    "            * fit intercept which equals true in stats models, we had to manually add a constant.\n",
    "            * The fit intercept parameter takes care precisely of that. If you don't want an intercept you can just set it to false.\n",
    "    * n_job\n",
    "        * ![Alt text](img/209.%20Simple%20Linear%20Regression%20with%20sklearn%20-%20A%20StatsModels-like%20Summary%20Table3.jpg)\n",
    "            * end jobs is a parameter used when we want to parallelize routines. \n",
    "            * By default, only one CPU is used.\n",
    "                * if you work on problems with lots and lots of data and have more than one CPU available, you can take advantage of this parameter by setting it to 2, 3, 5, and so on."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R-Squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40600391479679765"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reg.score(x,y) returns the R-Squared of a linear regression\n",
    "\n",
    "reg.score(x_matrix, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The result is 0.406, exactly the same as the one we found with stats models.\n",
    "    * ![Alt text](img/209.%20Simple%20Linear%20Regression%20with%20sklearn%20-%20A%20StatsModels-like%20Summary%20Table4.jpg)\n",
    "    * That's because this type of regression always has a single intercept. So both coeff and intercept value is the same (see below)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coeffiecients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00165569])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.coef_\n",
    "\n",
    "# the result same with SAT coef 0.0017 (see above table)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27504029966028076"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.intercept_\n",
    "\n",
    "# the result same with const coef 0.2750 (see above table)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got scalar array instead:\narray=1740.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Project\\Mrchmdani\\MyLearning\\Udemy\\The Data Science Course 2022 - Complete Data Science Bootcamp\\Udemy - Complete Data Science Bootcamp - Part5 - Advanced Statistical Methods in Pthon.ipynb Cell 209\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Project/Mrchmdani/MyLearning/Udemy/The%20Data%20Science%20Course%202022%20-%20Complete%20Data%20Science%20Bootcamp/Udemy%20-%20Complete%20Data%20Science%20Bootcamp%20-%20Part5%20-%20Advanced%20Statistical%20Methods%20in%20Pthon.ipynb#Y415sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Predict takes us arguments, the inputs we wanna predict and outputs the predictions according to the model.\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Project/Mrchmdani/MyLearning/Udemy/The%20Data%20Science%20Course%202022%20-%20Complete%20Data%20Science%20Bootcamp/Udemy%20-%20Complete%20Data%20Science%20Bootcamp%20-%20Part5%20-%20Advanced%20Statistical%20Methods%20in%20Pthon.ipynb#Y415sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m reg\u001b[39m.\u001b[39;49mpredict(\u001b[39m1740\u001b[39;49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\linear_model\\_base.py:355\u001b[0m, in \u001b[0;36mLinearModel.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m    342\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \u001b[39m    Predict using the linear model.\u001b[39;00m\n\u001b[0;32m    344\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[39m        Returns predicted values.\u001b[39;00m\n\u001b[0;32m    354\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 355\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decision_function(X)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\linear_model\\_base.py:338\u001b[0m, in \u001b[0;36mLinearModel._decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_decision_function\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m    336\u001b[0m     check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m--> 338\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, accept_sparse\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcoo\u001b[39;49m\u001b[39m\"\u001b[39;49m], reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    339\u001b[0m     \u001b[39mreturn\u001b[39;00m safe_sparse_dot(X, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoef_\u001b[39m.\u001b[39mT, dense_output\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercept_\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\base.py:535\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    534\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 535\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    536\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[0;32m    537\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\utils\\validation.py:892\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    889\u001b[0m \u001b[39mif\u001b[39;00m ensure_2d:\n\u001b[0;32m    890\u001b[0m     \u001b[39m# If input is scalar raise error\u001b[39;00m\n\u001b[0;32m    891\u001b[0m     \u001b[39mif\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 892\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    893\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mExpected 2D array, got scalar array instead:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39marray=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    894\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    895\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    896\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mif it contains a single sample.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    897\u001b[0m         )\n\u001b[0;32m    898\u001b[0m     \u001b[39m# If input is 1D raise error\u001b[39;00m\n\u001b[0;32m    899\u001b[0m     \u001b[39mif\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got scalar array instead:\narray=1740.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# Predict takes us arguments, the inputs we wanna predict and outputs the predictions according to the model.\n",
    "\n",
    "reg.predict(1740)\n",
    "\n",
    "# still don't get it how to fixes this\n",
    "# don't know where to reshape the value to 2D array\n",
    "# i download .ipynb from the lecture, but when i running the code still got problem with new scikitlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SAT\n",
       "0  1740\n",
       "1  1760"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's try this feature out and predict some more values. \n",
    "# I'll create a new data frame called new data. \n",
    "# It will be a Panda's data frame with data 1740 and 1760 and a column name SAT.\n",
    "\n",
    "new_data = pd.DataFrame(data=[1740, 1760], columns=['SAT'])\n",
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cilit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\base.py:402: UserWarning: X has feature names, but LinearRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3.15593751, 3.18905127])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.predict(new_data)\n",
    "\n",
    "# The result is an array with the two predictions, 3.16 and 3.19."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cilit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\base.py:402: UserWarning: X has feature names, but LinearRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAT</th>\n",
       "      <th>Predicted_GPA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1740</td>\n",
       "      <td>3.155938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1760</td>\n",
       "      <td>3.189051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SAT  Predicted_GPA\n",
       "0  1740       3.155938\n",
       "1  1760       3.189051"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding information directly into the original data frame\n",
    "\n",
    "new_data['Predicted_GPA'] = reg.predict(new_data)\n",
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x, y)\n",
    "yhat = reg.coef_ * x_matrix + reg.intercept_\n",
    "# yhat = 0.0017 * x + 0.275\n",
    "fig = plt.plot(x, yhat, lw=4, c='orange', label='regression line')\n",
    "plt.xlabel('SAT', fontsize=20)\n",
    "plt.ylabel('GPA', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 210. A Note on Normalization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/14390346#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The Wikipedia article on Feature scaling: \n",
    "    * https://en.wikipedia.org/wiki/Feature_scaling\n",
    "2. This article on L1-norm and L2-norm: \n",
    "    * http://www.chioka.in/differences-between-the-l1-norm-and-the-l2-norm-least-absolute-deviations-and-least-squares/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 211. Simple Linear Regression with sklearn - Exercise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/14390356#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 212. Multiple Linear Regression with sklearn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/14390366#overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAT</th>\n",
       "      <th>GPA</th>\n",
       "      <th>Rand 1,2,3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1714</td>\n",
       "      <td>2.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1664</td>\n",
       "      <td>2.52</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1760</td>\n",
       "      <td>2.54</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1685</td>\n",
       "      <td>2.74</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1693</td>\n",
       "      <td>2.83</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SAT   GPA  Rand 1,2,3\n",
       "0  1714  2.40           1\n",
       "1  1664  2.52           3\n",
       "2  1760  2.54           3\n",
       "3  1685  2.74           3\n",
       "4  1693  2.83           2"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data\n",
    "\n",
    "data = pd.read_csv('csv/1.02. Multiple linear regression.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAT</th>\n",
       "      <th>GPA</th>\n",
       "      <th>Rand 1,2,3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>84.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>84.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1845.273810</td>\n",
       "      <td>3.330238</td>\n",
       "      <td>2.059524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>104.530661</td>\n",
       "      <td>0.271617</td>\n",
       "      <td>0.855192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1634.000000</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1772.000000</td>\n",
       "      <td>3.190000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1846.000000</td>\n",
       "      <td>3.380000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1934.000000</td>\n",
       "      <td>3.502500</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2050.000000</td>\n",
       "      <td>3.810000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               SAT        GPA  Rand 1,2,3\n",
       "count    84.000000  84.000000   84.000000\n",
       "mean   1845.273810   3.330238    2.059524\n",
       "std     104.530661   0.271617    0.855192\n",
       "min    1634.000000   2.400000    1.000000\n",
       "25%    1772.000000   3.190000    1.000000\n",
       "50%    1846.000000   3.380000    2.000000\n",
       "75%    1934.000000   3.502500    3.000000\n",
       "max    2050.000000   3.810000    3.000000"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we already know that the SAT score is a good predictor of GPA both from last section and the simple linear regression with sklearn we created. \n",
    "* Rand 1,2,3 was a variable which randomly assigned one, two, or three to each sample. \n",
    "    * Here, it is helpful to note that **sample** is the machine learning word for observation.\n",
    "        * example: this data set have 84 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creting regression\n",
    "\n",
    "x = data[['SAT', 'Rand 1,2,3']]\n",
    "y = data['GPA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'copy_X': True, 'fit_intercept': True, 'n_jobs': None, 'positive': False}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# regression\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(x,y).get_params()\n",
    "\n",
    "# Notice that the cell executed without an error, because we already assign x for multiple array\n",
    "# sklearn is optimized for multiple linear regression\n",
    "# We needed no more than two lines of code to create a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00165354, -0.00826982])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will display the coefficients and intercept of our model\n",
    "\n",
    "reg.coef_\n",
    "\n",
    "# reg.coef_ displays an array with two elements, the coefficient of the SAT score and the coefficient of the Rand 1,2,3 variable. \n",
    "# They are ordered in the way we fed them.\n",
    "\n",
    "# we realized that 0.0017 corresponds to the SAT score, while minus 0.0083 to Rand 1,2,3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29603261264909486"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.intercept_\n",
    "\n",
    "# reg.intercept shows us that the intercept of this model is 0.296. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ![Alt text](img/212.%20Multiple%20Linear%20Regression%20with%20sklearn1.jpg)\n",
    "    * By comparing these results with the StatsModels summary, we confirm that the two packages yield identical results with StatsModels."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 213. Calculating the Adjusted R-Squared in sklearn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/14390378#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **The R squared** is one of the most common measures of goodness of fit. In other words, it **is a universal measure to evaluate how well linear regressions fair and compare**.\n",
    "* There is a very intuitive method called score which can help us with that. Depending on the type of regression you are performing a regression score will have a different meaning.\n",
    "    * ![Alt text](img/213.%20Calculating%20the%20Adjusted%20R-Squared%20in%20sklearn1.jpg)\n",
    "    * reg.score(x,y)\n",
    "        * returns the R-squared of a linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4066811952814282"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the R-Squared\n",
    "\n",
    "reg.score(x, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How do I calculate the Adjusted R-squared score using scikit-learn?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ![Alt text](img/213.%20Calculating%20the%20Adjusted%20R-Squared%20in%20sklearn2.jpg)\n",
    "    * ![Alt text](img/213.%20Calculating%20the%20Adjusted%20R-Squared%20in%20sklearn3.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39203134825134"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OPTION 1\n",
    "# Code for adjusted R-squared\n",
    "\n",
    "r2 = reg.score(x, y)\n",
    "n = x.shape[0]\n",
    "p = x.shape[1]\n",
    "\n",
    "adjusted_r2 = 1 - (1-r2) * (n-1) / (n-p-1)\n",
    "adjusted_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39203134825134"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OPTION 2\n",
    "# Code for adjusted R-squared\n",
    "\n",
    "r2_formula = 1 - ( 1 - reg.score(x, y) ) * ( len(y) - 1 ) / ( len(y) - x.shape[1] - 1 )\n",
    "r2_formula"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 214. Calculating the Adjusted R-Squared in sklearn - Exercise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/14390386#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 215. Feature Selection (F-regression)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/14390402#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How to detect the variables which are unneeded in a model.\n",
    "    * ![Alt text](img/215.%20Feature%20Selection%20(F-regression)1.jpg)\n",
    "        * There's actually a whole process created for that purpose. It is called **feature selection**.\n",
    "        * Feature selection is a very important procedure in machine learning as it simplifies models which makes them much easier to interpret by data scientists.\n",
    "        * Through this process, we gain improved speed and often prevent a series of other unwanted issues arising from having too many features.\n",
    "    * ![Alt text](img/215.%20Feature%20Selection%20(F-regression)2.jpg)\n",
    "        * In fact, when we are dealing with stats models, we were given the P values of the features which we later used to determine whether the independent variables were relevant for the model.\n",
    "        * if a variable has a P-value above 0.05, we can disregard it.\n",
    "\n",
    "* How can we find the P-values in SK learn?\n",
    "    * Well, this is one of the cases where SK learn manifests itself as a machine learning package, rather than a statistical one. There is no built in method we can call to get them. Therefore, we must find a workaround. A very close concept is available with the feature selection module from SK Learn. It is called **F regression**.\n",
    "    * F regression creates simple linear regressions of each feature and the dependent variable. \n",
    "        * For our GPA SAT example, it would translate into two regressions. \n",
    "            * One where we predict GPA with SAT, and \n",
    "            * one where we predict GPA with Rand 123. \n",
    "        * Then the method would calculate the F statistic for each of those regressions and return the respective P-values. \n",
    "            * If there were 50 features, 50 simple regressions would be created. \n",
    "        * Note that for a simple linear regression, the P-value of the F statistic coincides with the P-value of the only independent variable.\n",
    "            * ![Alt text](img/215.%20Feature%20Selection%20(F-regression)3.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Feature Selection Regression\n",
    "\n",
    "from sklearn.feature_selection import f_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([56.04804786,  0.17558437]), array([7.19951844e-11, 6.76291372e-01]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_regression(x, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* there are two output arrays:\n",
    "    * ![Alt text](img/215.%20Feature%20Selection%20(F-regression)4.jpg)\n",
    "    * The first one contains the F statistics for each of the regressions, \n",
    "    * while the second the corresponding P-values.\n",
    "\n",
    "* There is scientific notation on output:\n",
    "    * Here's how to interpret them.\n",
    "        * ![Alt text](img/215.%20Feature%20Selection%20(F-regression)5.jpg)\n",
    "        * E minus 11 means times 10 to the power of minus 11, or divided by 10 to the power of 11.\n",
    "        * Similarly, E minus one means times 10 to the power of one, or simply divided by 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.19951844e-11, 6.76291372e-01])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating variable P-Value\n",
    "\n",
    "p_values = f_regression(x,y)[1]\n",
    "p_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.   , 0.676])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rounding the p-value\n",
    "# we can choose the number of digits after the dot we want to round to. \n",
    "# Three is completely enough for P-value interpretation.\n",
    "\n",
    "p_values.round(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The first one refers to the first column of X, while the second one to the second. (x1, x2)\n",
    "    * ![Alt text](img/215.%20Feature%20Selection%20(F-regression)6.jpg)\n",
    "        * the P value of SAT is 0.000\n",
    "        * and that RAND 1, 2, 3 is 0.676.\n",
    "    * Conclusion:\n",
    "        * With your prior regression analysis experience, you can easily determine that **SAT is a useful variable, while rand 1, 2, 3 is useless**.\n",
    "\n",
    "* What should be noted though, is that these are the univariate p-values or the p-values reached from simple linear models. \n",
    "    * They do not reflect the interconnection of the features in our multiple linear regression. Therefore, f regression should be used with caution, as it is one bit too simplistic for more complicated problems."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 216. A Note on Calculation of P-values with sklearn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/14390418#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A Note on Calculation of P-values with sklearn\n",
    "    * As suggested in the previous lecture, the F-regression does not take into account the interrelation of the features.\n",
    "    * A not so simple fix for that is to ammend the LinearRegression() class.\n",
    "    * You can find the relevant code with comments with this article.\n",
    "    * Note that the results will be identical to those of StatsModels."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 217. Creating a Summary Table with P-values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/14390434#overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SAT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rand 1,2,3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Features\n",
       "0         SAT\n",
       "1  Rand 1,2,3"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating Data Frame\n",
    "\n",
    "# reg_summary = pd.DataFrame(data=['SAT',  'Rand 1,2,3'], columns=['Features'])\n",
    "\n",
    "# If we want to parameterize our code, we would not write the names of the two features, \n",
    "# but rather refer to the column values of the data frame, X.\n",
    "\n",
    "reg_summary = pd.DataFrame(data= x.columns.values, columns=['Features'])\n",
    "reg_summary\n",
    "\n",
    "# After executing the cell, we find that the result is identical.\n",
    "# However, much more preferable for models with dozens or hundreds of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>coefficients</th>\n",
       "      <th>p-values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SAT</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rand 1,2,3</td>\n",
       "      <td>-0.008270</td>\n",
       "      <td>0.676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Features  coefficients  p-values\n",
       "0         SAT      0.001654     0.000\n",
       "1  Rand 1,2,3     -0.008270     0.676"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding new column to Data Frame called Coefficients and P-Values\n",
    "\n",
    "reg_summary['coefficients'] = reg.coef_\n",
    "reg_summary['p-values'] = p_values.round(3)\n",
    "reg_summary\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Once more, we conclude that Rand 123 does not contribute to our model and should be removed.\n",
    "    * ![Alt text](img/217.%20Creating%20a%20Summary%20Table%20with%20P-values.jpg)\n",
    "\n",
    "* P values are one of the best ways to determine if a variable is redundant, but they provide no information whatsoever about how useful a variable is. That is, two variables may both have a p value of 0.000, making them relevant, but this does not mean they are equally important."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 218. Multiple Linear Regression - Exercise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/14390444#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 219. Feature Scaling (Standardization)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/14390448#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The most common problem when working with numerical data is the difference in magnitudes.\n",
    "    * How to fix:\n",
    "        * standardization also known as feature scaling\n",
    "        * normalization.\n",
    "\n",
    "* Standardization, or feature scaling is the process of transforming the data we are working with into a standard scale.\n",
    "    * ![Alt text](img/219.%20Feature%20Scaling%20(Standardization)1.jpg)\n",
    "        * This translates to subtracting the mean and dividing by the standard deviation.\n",
    "        * In this way, regardless of the data set we will always obtain a distribution with a mean of zero and a standard deviation of one which could easily be proven.\n",
    "\n",
    "* Example:\n",
    "    * ![Alt text](img/219.%20Feature%20Scaling%20(Standardization)1.jpg)\n",
    "    * ![Alt text](img/219.%20Feature%20Scaling%20(Standardization)3.jpg)\n",
    "\n",
    "* Our plan for this lecture is to scale them using the Standard Scaler module from sklearn.\n",
    "    * ![Alt text](img/219.%20Feature%20Scaling%20(Standardization)4.jpg)\n",
    "    * StandardScaler()\n",
    "        * a preprocessing module used to standardize (or scale) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAT</th>\n",
       "      <th>GPA</th>\n",
       "      <th>Rand 1,2,3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1714</td>\n",
       "      <td>2.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1664</td>\n",
       "      <td>2.52</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1760</td>\n",
       "      <td>2.54</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1685</td>\n",
       "      <td>2.74</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1693</td>\n",
       "      <td>2.83</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SAT   GPA  Rand 1,2,3\n",
       "0  1714  2.40           1\n",
       "1  1664  2.52           3\n",
       "2  1760  2.54           3\n",
       "3  1685  2.74           3\n",
       "4  1693  2.83           2"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data\n",
    "\n",
    "data = pd.read_csv('csv/1.02. Multiple linear regression.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAT</th>\n",
       "      <th>GPA</th>\n",
       "      <th>Rand 1,2,3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>84.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>84.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1845.273810</td>\n",
       "      <td>3.330238</td>\n",
       "      <td>2.059524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>104.530661</td>\n",
       "      <td>0.271617</td>\n",
       "      <td>0.855192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1634.000000</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1772.000000</td>\n",
       "      <td>3.190000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1846.000000</td>\n",
       "      <td>3.380000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1934.000000</td>\n",
       "      <td>3.502500</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2050.000000</td>\n",
       "      <td>3.810000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               SAT        GPA  Rand 1,2,3\n",
       "count    84.000000  84.000000   84.000000\n",
       "mean   1845.273810   3.330238    2.059524\n",
       "std     104.530661   0.271617    0.855192\n",
       "min    1634.000000   2.400000    1.000000\n",
       "25%    1772.000000   3.190000    1.000000\n",
       "50%    1846.000000   3.380000    2.000000\n",
       "75%    1934.000000   3.502500    3.000000\n",
       "max    2050.000000   3.810000    3.000000"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we already know that the SAT score is a good predictor of GPA both from last section and the simple linear regression with sklearn we created. \n",
    "* Rand 1,2,3 was a variable which randomly assigned one, two, or three to each sample. \n",
    "    * Here, it is helpful to note that **sample** is the machine learning word for observation.\n",
    "        * example: this data set have 84 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creting regression\n",
    "\n",
    "x = data[['SAT', 'Rand 1,2,3']]\n",
    "y = data['GPA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Standardization\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# The object we just created will be used to scale our data. \n",
    "# In other words, it will subtract the mean and divide by the standard deviation from each point feature-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'copy': True, 'with_mean': True, 'with_std': True}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit input data\n",
    "\n",
    "scaler.fit(x).get_params()\n",
    "\n",
    "# This line will calculate the mean and standard deviation of each feature.\n",
    "# This information will be stored in the scaler object. \n",
    "# So it won't be an empty object anymore.\n",
    "# It will contain information about the mean and standard deviation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Step above is extremely important. \n",
    "* Whenever you get new data, you will know that the standardization information is contained in the scaler.\n",
    "    * ![Alt text](img/219.%20Feature%20Scaling%20(Standardization)5.jpg)\n",
    "\n",
    "* we have the information but the inputs are still unscaled. We have just prepared the scaling mechanism.\n",
    "    * ![Alt text](img/219.%20Feature%20Scaling%20(Standardization)6.jpg)\n",
    "    * In order to apply it, we must use another method called transform.\n",
    "    * As the name suggests, transform, transforms the unscaled inputs using the information contained in scaler. \n",
    "    * In simple words, we subtract the mean and divide by the standard deviation for each feature.\n",
    "    * Whenever we get new data we will just apply scaler.transform new data to reach the same transformation as we just did.\n",
    "        * ![Alt text](img/219.%20Feature%20Scaling%20(Standardization)7.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.26338288, -1.24637147],\n",
       "       [-1.74458431,  1.10632974],\n",
       "       [-0.82067757,  1.10632974],\n",
       "       [-1.54247971,  1.10632974],\n",
       "       [-1.46548748, -0.07002087],\n",
       "       [-1.68684014, -1.24637147],\n",
       "       [-0.78218146, -0.07002087],\n",
       "       [-0.78218146, -1.24637147],\n",
       "       [-0.51270866, -0.07002087],\n",
       "       [ 0.04548499,  1.10632974],\n",
       "       [-1.06127829,  1.10632974],\n",
       "       [-0.67631715, -0.07002087],\n",
       "       [-1.06127829, -1.24637147],\n",
       "       [-1.28263094,  1.10632974],\n",
       "       [-0.6955652 , -0.07002087],\n",
       "       [ 0.25721362, -0.07002087],\n",
       "       [-0.86879772,  1.10632974],\n",
       "       [-1.64834403, -0.07002087],\n",
       "       [-0.03150724,  1.10632974],\n",
       "       [-0.57045283,  1.10632974],\n",
       "       [-0.81105355,  1.10632974],\n",
       "       [-1.18639066,  1.10632974],\n",
       "       [-1.75420834,  1.10632974],\n",
       "       [-1.52323165, -1.24637147],\n",
       "       [ 1.23886453, -1.24637147],\n",
       "       [-0.18549169, -1.24637147],\n",
       "       [-0.5608288 , -1.24637147],\n",
       "       [-0.23361183,  1.10632974],\n",
       "       [ 1.68156984, -1.24637147],\n",
       "       [-0.4934606 , -0.07002087],\n",
       "       [-0.73406132, -1.24637147],\n",
       "       [ 0.85390339, -1.24637147],\n",
       "       [-0.67631715, -1.24637147],\n",
       "       [ 0.09360513,  1.10632974],\n",
       "       [ 0.33420585, -0.07002087],\n",
       "       [ 0.03586096, -0.07002087],\n",
       "       [-0.35872421,  1.10632974],\n",
       "       [ 1.04638396,  1.10632974],\n",
       "       [-0.65706909,  1.10632974],\n",
       "       [-0.13737155, -0.07002087],\n",
       "       [ 0.18984542,  1.10632974],\n",
       "       [ 0.04548499, -1.24637147],\n",
       "       [ 1.1618723 ,  1.10632974],\n",
       "       [-1.37887123, -1.24637147],\n",
       "       [ 1.39284898, -1.24637147],\n",
       "       [ 0.76728713, -0.07002087],\n",
       "       [-0.20473975, -0.07002087],\n",
       "       [ 1.06563201, -1.24637147],\n",
       "       [ 0.11285319, -1.24637147],\n",
       "       [ 1.28698467,  1.10632974],\n",
       "       [-0.41646838,  1.10632974],\n",
       "       [ 0.09360513, -1.24637147],\n",
       "       [ 0.59405462, -0.07002087],\n",
       "       [-2.03330517, -0.07002087],\n",
       "       [ 0.32458182, -1.24637147],\n",
       "       [ 0.40157405, -1.24637147],\n",
       "       [-1.10939843, -0.07002087],\n",
       "       [ 1.03675993, -1.24637147],\n",
       "       [-0.61857297, -0.07002087],\n",
       "       [ 0.44007016, -0.07002087],\n",
       "       [ 1.14262424, -1.24637147],\n",
       "       [-0.35872421,  1.10632974],\n",
       "       [ 0.45931822,  1.10632974],\n",
       "       [ 1.88367444,  1.10632974],\n",
       "       [ 0.45931822, -1.24637147],\n",
       "       [-0.12774752, -0.07002087],\n",
       "       [ 0.04548499,  1.10632974],\n",
       "       [ 0.85390339, -0.07002087],\n",
       "       [ 0.15134931, -0.07002087],\n",
       "       [ 0.8250313 ,  1.10632974],\n",
       "       [ 0.84427936,  1.10632974],\n",
       "       [-0.64744506, -1.24637147],\n",
       "       [ 1.24848856, -1.24637147],\n",
       "       [ 0.85390339,  1.10632974],\n",
       "       [ 1.69119387,  1.10632974],\n",
       "       [ 1.6334497 ,  1.10632974],\n",
       "       [ 1.46021718, -1.24637147],\n",
       "       [ 1.68156984, -0.07002087],\n",
       "       [-0.02188321,  1.10632974],\n",
       "       [ 0.87315144,  1.10632974],\n",
       "       [-0.33947615, -1.24637147],\n",
       "       [ 1.3639769 ,  1.10632974],\n",
       "       [ 1.12337618, -1.24637147],\n",
       "       [ 1.97029069, -0.07002087]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaled = scaler.transform(x)\n",
    "x_scaled"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 220. Feature Selection through Standardization of Weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/14390454#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When we calculated the coefficients of this regression without standardization we could not immediately see the effect of each variable on the output.\n",
    "    * ![Alt text](img/220.%20Feature%20Selection%20through%20Standardization%20of%20Weights1.jpg)\n",
    "        * The reason is that SAT is ranging between 600 and 2400 while Rand 1, 2, 3, between one, two, and three. \n",
    "        * Their respective coefficients were 0.0017 and minus 0.0083.\n",
    "            * So it seems like Rand 1, 2, 3 has a bigger coefficient hence a greater impact.\n",
    "        * However, this conclusion could be wrong. Since the SAT score has a much greater magnitude it is much more important.\n",
    "            *  ![Alt text](img/220.%20Feature%20Selection%20through%20Standardization%20of%20Weights2.jpg)\n",
    "                * However, by looking at the coefficients alone one might be deceived. \n",
    "                * This issue is overcome through feature scaling. Having all inputs with the same magnitude allows us to compare their impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'copy_X': True, 'fit_intercept': True, 'n_jobs': None, 'positive': False}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Regression with scaled features\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(x_scaled, y).get_params()\n",
    "\n",
    "# As usual, we will get nothing more than a line confirming that the regression had been successfully performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.17181389, -0.00703007])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To assess the actual difference, we can look at the coefficients and the intercept.\n",
    "\n",
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.330238095238095"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intercept</td>\n",
       "      <td>3.330238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SAT</td>\n",
       "      <td>0.171814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rand 1,2,3</td>\n",
       "      <td>-0.007030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Features    Weight\n",
       "0   Intercept  3.330238\n",
       "1         SAT  0.171814\n",
       "2  Rand 1,2,3 -0.007030"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating Summary Table\n",
    "\n",
    "reg_summary = pd.DataFrame([['Intercept'], ['SAT'], ['Rand 1,2,3']], columns=['Features'])\n",
    "reg_summary['Weight'] = reg.intercept_, reg.coef_[0], reg.coef_[1]\n",
    "reg_summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Weight is machine learning word for coefficients\n",
    "    * ![Alt text](img/220.%20Feature%20Selection%20through%20Standardization%20of%20Weights3.jpg)\n",
    "        * The name comes from the fact that the bigger the weight the bigger the impact of the feature on the regression. It carries weight on the result.\n",
    "* Intercept is called bias on machine learning context.\n",
    "    * ![Alt text](img/220.%20Feature%20Selection%20through%20Standardization%20of%20Weights4.jpg)\n",
    "        *  The idea is that the intercept is nothing but a number which adjusts our regression with some constant. But if we need to adjust our regression with some constant then the regression is biased by that number.\n",
    "        \n",
    "* Using these new naming conventions we can conveniently distinguish between a regular coefficient summary table and this new one with a standardized coefficients or weights.\n",
    "    * ![Alt text](img/220.%20Feature%20Selection%20through%20Standardization%20of%20Weights5.jpg)\n",
    "\n",
    "* What about the interpretation of these weights? \n",
    "    * The closer a weight is to zero, the smaller it's impact.\n",
    "    * The bigger the weight, the bigger it's impact.\n",
    "        * In the example, the weight of SAT is 0.17, which is much much bigger than that of Rand 1, 2, 3 standing at merely minus 0.007. \n",
    "        * This brings us to feature selection through standardization, we can clearly see that Rand 1,2, 3 barely contributes to our outputs, if at all.\n",
    "        * It will make little difference if we remove it from the model or leave it there with a weight of almost zero. As zero times any value is equal to zero.\n",
    "\n",
    "* When we perform feature scaling, we don't really care if a useless variable is there or not.\n",
    "    * This is also one of the main reasons why Sklearn does not natively support P values. Since most ML practitioners perform some kind of feature scaling before fitting a model, we don't really need to identify the worst performing features. They are automatically penalized by having a very small weight."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 221. Predicting with the Standardized Coefficients"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/14390458#overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAT</th>\n",
       "      <th>Rand 1,2,3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1700</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1800</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SAT  Rand 1,2,3\n",
       "0  1700           2\n",
       "1  1800           1"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = pd.DataFrame(data=[[1700, 2], [1800, 1]], columns=['SAT', 'Rand 1,2,3'])\n",
    "new_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is crucial that our new data, data frame is arranged in the exact same way as our input data, X.\n",
    "    * ![Alt text](img/221.%20Predicting%20with%20the%20Standardized%20Coefficients1.jpg)\n",
    "\n",
    "* After we confirm this we can proceed to predicting the new values.\n",
    "    * We can simply call the predict method on the regression and then specify the new inputs as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cilit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\base.py:402: UserWarning: X has feature names, but LinearRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([295.39979563, 312.58821497])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.predict(new_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The result that we get is quite confusing. 295.4 and 312.6.\n",
    "    * This is not even a valid GPA. \n",
    "    * This happened because our regression model was trained on standardized inputs.\n",
    "        * ![Alt text](img/221.%20Predicting%20with%20the%20Standardized%20Coefficients2.jpg)\n",
    "            * It expects values that are of the same magnitude as the ones used in the training process.\n",
    "    \n",
    "* We can add that it also must be standardized in the same way, with the same mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.39811928, -0.07002087],\n",
       "       [-0.43571643, -1.24637147]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data_scaled = scaler.transform(new_data)\n",
    "new_data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.09051403, 3.26413803])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.predict(new_data_scaled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here's the result, it is of the magnitude we anticipated. \n",
    "    * Our first student is predicted to have a GPA of 3.09 while the second 3.26.\n",
    "\n",
    "* What would happen if we decided to remove the rand. 1, 2, 3 feature theory suggests that nothing should change, right?\n",
    "    * Let's try it\n",
    "        * First, we must create a new regression or Reg Simple as it is a simple linear regression.\n",
    "        * Next, we must declare the inputs. Let's create a new variable called x_simple which will contain all observations from X scaled from last lecture, but only referring to the SAT column or column zero.\n",
    "            * If you remember, SK learn would return an error if the inputs are not of matrix form so let's make it a matrix with the reshape method.\n",
    "        * Next, we fit the regression with inputs X simple matrix and outputs Y.\n",
    "            * Once the regression is fitted, we can proceed to predicting the new data.\n",
    "        * Reg Simple dot Predict with argument only the first column of the scaled new data.\n",
    "            * It is crucial that we only feed the S A T score because this regression was trained only on it. \n",
    "            * Moreover, once again, it must be reshaped so the code executes properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'copy_X': True, 'fit_intercept': True, 'n_jobs': None, 'positive': False}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_simple = LinearRegression()\n",
    "x_simple_matrix = x_scaled[:,0].reshape(-1,1)\n",
    "reg_simple.fit(x_simple_matrix, y).get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.08970998, 3.25527879])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_simple.predict(new_data_scaled[:,0].reshape(-1, 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Result & Compare\n",
    "    * ![Alt text](img/221.%20Predicting%20with%20the%20Standardized%20Coefficients3.jpg)\n",
    "        * The predicted GPA is slightly different,but actually if we  round up to two digits after the dot we get the exact same results 3.09 and 3.26.\n",
    "        * This finding shows us why the developers of SK Learn have decided that P values are not needed.\n",
    "        * When we apply feature scaling it often does not affect the final result if We keep or leave out in significant features Their weights will be so close to zero that they will barely influence the predictions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 222. Feature Scaling (Standardization) - Exercise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/14390462#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 223. Underfitting and Overfitting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/14390466#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* One of the most commonly asked questions at data science interviews is about overfitting. \"What's overfitting and how do we deal with it?\"\n",
    "    \n",
    "* **There are two concepts that are interrelated, underfitting and overfitting.**\n",
    "    * They go together and  Understanding one helps us understand the other and vice versa.\n",
    "    * ![Alt text](img/223.%20Underfitting%20and%20Overfitting1.jpg)\n",
    "        * overfitting means our regression has focused on the particular data set so much it has missed the point,\n",
    "        * underfitting, means the model has not captured the underlying logic of the data.\n",
    "    * ![Alt text](img/223.%20Underfitting%20and%20Overfitting2.jpg)\n",
    "        * Here we can see several data points. A good algorithm would result in a model that looks like graph above.\n",
    "        * It is not perfect, but is very close to the actual relationship.\n",
    "    * Underfitted\n",
    "        * ![Alt text](img/223.%20Underfitting%20and%20Overfitting3.jpg)\n",
    "            * Linear model would be an underfitting model. It provides an answer, but does not capture the underlying logic of the data. It doesn't have strong predictive power. \n",
    "            * Underfitted models are clumsy and have a low accuracy.\n",
    "            * You will quickly realize that either there are no relationships to be found, or you need a different model.\n",
    "    * Overfitted\n",
    "        * ![Alt text](img/223.%20Underfitting%20and%20Overfitting4.jpg)\n",
    "            * Overfitting refers to models that are so super good at modeling the data that they fit, or at least come very near each observation. The problem is that the random noise is captured inside an overfitted model.\n",
    "    * ![Alt text](img/223.%20Underfitting%20and%20Overfitting5.jpg)\n",
    "        * Underfitting is easy to spot. \n",
    "            * You have almost no accuracy whatsoever.\n",
    "        * Overfitting is much harder.\n",
    "            * As the accuracy of the model seems outstanding.\n",
    "            * There is one popular solution to overfitting. \n",
    "                * We can split our initial data set into two, training and test. Splits like 90% training and 10% test, or 80 20 are common.\n",
    "                * We create the regression on the training data. After we have the coefficients we test the model on the test data by assessing the accuracy.\n",
    "                    * The whole point is that the model has never seen the test data set. Therefore, it cannot overfit on it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 224. Train - Test Split Explained"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/14390476#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* STEP BY STEP\n",
    "    * First, import NumPy then from Sklearn, model selection and import train_test_split\n",
    "    * Now, generate some data we are going to split.\n",
    "        * Create two arrays: A and B.\n",
    "            * ![Alt text](img/224.%20Train%20-%20Test%20Split%20Explained1.jpg)\n",
    "                * Arange is equivalent to the built-in Python function range but places the generated values in an array rather than a list.\n",
    "            * Let A be equal to np arange with arguments one and 101 and the result is an array with elements ranging from one to 100 included.\n",
    "                * It is very important that the values inside are arranged. This will help us a great deal to follow what the splitting process has accomplished.\n",
    "            * Now, create B which will range from 501 to 600.\n",
    "    * Then split the data using train_test_split\n",
    "        * ![Alt text](img/224.%20Train%20-%20Test%20Split%20Explained2.jpg)\n",
    "        * It takes an array and splits it into two arrays.\n",
    "            * ![Alt text](img/224.%20Train%20-%20Test%20Split%20Explained3.jpg)\n",
    "                * In general, we would prefer storing the result in dedicated variables so we can write a_train, a_test equals train_test_split of a.\n",
    "                * The first array is always considered to be the training array while the second, the testing one.\n",
    "    * Let's explore the result.\n",
    "        * First, it makes sense to check the shapes of the two variables.\n",
    "            * A_train has a length of 75 while a_test, a length of 25. \n",
    "            * Here are the respective arrays. There are two important observations that we can spot right away.\n",
    "                * First, the default split is 75, 25.\n",
    "                * Second, both arrays are shuffled.\n",
    "                    * They used to consist of numbers ordered from one to a 100 but are now completely randomized.\n",
    "                    * In fact, these are the default settings of train_test_split. 75, 25 is not an optimal split for my work as I really don't like dedicating so much of my data to testing.\n",
    "        * To amend this, we can use an argument called test size and set it to a float between zero and one. Therefore, to achieve an 80/20 split, we need to simply include test_size equals 0.2.\n",
    "            * ![Alt text](img/224.%20Train%20-%20Test%20Split%20Explained4.jpg)\n",
    "        * The main problem is that each time we split the data, we will get different training and testing datasets. So, if you are working on your models at home, you'd be creating a different regression on different data each time you run the cells. In the best case scenario, you would like to have shuffle data but shuffled in the same way every time. Fortunately, Sklearn has a random state argument.\n",
    "            * ![Alt text](img/224.%20Train%20-%20Test%20Split%20Explained5.jpg)\n",
    "                * add the random state argument and set it to 42. \n",
    "                    * 42 is the conventionally used number by the community. A kind of an internety joke. \n",
    "                * If we try rerunning the code again and again, we would always get the exact same shuffled split.\n",
    "        * SPECIAL CASE:\n",
    "            * Sometimes when working with time series data for instance, the order of the array is of utmost importance. In these cases, we prefer our dataset to be split without being shuffled. There's an argument called shuffle which is set to true by default. Changing that to false would yield a very intuitive result.\n",
    "                * ![Alt text](img/224.%20Train%20-%20Test%20Split%20Explained6.jpg)\n",
    "    * Finally, we must note that this method has another convenience. We can split more than one array at the same time.\n",
    "        * ![Alt text](img/224.%20Train%20-%20Test%20Split%20Explained7.jpg)\n",
    "        * ![Alt text](img/224.%20Train%20-%20Test%20Split%20Explained8.jpg)\n",
    "            * Here's where our B array comes into play. We can simply add B as an argument and include two new variables to store the returned arrays. We'll call them B_train and B_test."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate some data we are going to split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n",
       "        27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n",
       "        40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,\n",
       "        53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,\n",
       "        66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,\n",
       "        79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
       "        92,  93,  94,  95,  96,  97,  98,  99, 100])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(1,101)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513,\n",
       "       514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526,\n",
       "       527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539,\n",
       "       540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552,\n",
       "       553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565,\n",
       "       566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578,\n",
       "       579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591,\n",
       "       592, 593, 594, 595, 596, 597, 598, 599, 600])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.arange(501, 601)\n",
    "b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 93,  28,  38,  18,  24,   7,  54,  58,  96,  26,  36,  97,  39,\n",
       "         67,  99,  41,  90,  64,  17,   3,  37,  81,   6,  10,  98,  47,\n",
       "         61,  68,  92,  16,  27,  85,  83,  12,  48,  52,  51,  62,  31,\n",
       "         77,  23,  53,  71,   1,   2,  66,  20,  86,  69,  19, 100,  76,\n",
       "         74,  30,   8,  65,  73,  72,  22,   5,  45,  25,   9,  33,  32,\n",
       "         46,  57,  78,  89,  29,  13,  55,  60,  40,  15]),\n",
       " array([43, 14, 35, 80, 21,  4, 42, 49, 88, 82, 34, 94, 91, 87, 79, 75, 44,\n",
       "        56, 84, 63, 50, 95, 59, 11, 70])]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_split(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_train, a_test, b_train, b_test = train_test_split(a, b, test_size=0.2, random_state=42) \n",
    "#changing split from default 75/25 to 80/20 using test_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explorer the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((80,), (20,))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_train.shape, a_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 56,  89,  27,  43,  70,  16,  41,  97,  10,  73,  12,  48,  86,\n",
       "        29,  94,   6,  67,  66,  36,  17,  50,  35,   8,  96,  28,  20,\n",
       "        82,  26,  63,  14,  25,   4,  18,  39,   9,  79,   7,  65,  37,\n",
       "        90,  57, 100,  55,  44,  51,  68,  47,  69,  62,  98,  80,  42,\n",
       "        59,  49,  99,  58,  76,  33,  95,  60,  64,  85,  38,  30,   2,\n",
       "        53,  22,   3,  24,  88,  92,  75,  87,  83,  21,  61,  72,  15,\n",
       "        93,  52])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([84, 54, 71, 46, 45, 40, 23, 81, 11,  1, 19, 31, 74, 34, 91,  5, 77,\n",
       "       78, 13, 32])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((80,), (20,))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_train.shape, b_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([556, 589, 527, 543, 570, 516, 541, 597, 510, 573, 512, 548, 586,\n",
       "       529, 594, 506, 567, 566, 536, 517, 550, 535, 508, 596, 528, 520,\n",
       "       582, 526, 563, 514, 525, 504, 518, 539, 509, 579, 507, 565, 537,\n",
       "       590, 557, 600, 555, 544, 551, 568, 547, 569, 562, 598, 580, 542,\n",
       "       559, 549, 599, 558, 576, 533, 595, 560, 564, 585, 538, 530, 502,\n",
       "       553, 522, 503, 524, 588, 592, 575, 587, 583, 521, 561, 572, 515,\n",
       "       593, 552])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([584, 554, 571, 546, 545, 540, 523, 581, 511, 501, 519, 531, 574,\n",
       "       534, 591, 505, 577, 578, 513, 532])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* By exploring the shapes and the arrays themselves, we reach a consistent result.\n",
    "    * A consists of the ordered sequence of the numbers from 1 to a 100 \n",
    "    * while B from 501 to 600.\n",
    "\n",
    "* Therefore, we can say that the number one from A matches with 501 from B, 25 from A matches with 525 from B and so on. When we split A and B using the train_test_split, their elements are shuffled in the same way. Exploring A_train and B_train for instance shows us that they are indeed matching.\n",
    "    * ![Alt text](img/224.%20Train%20-%20Test%20Split%20Explained9.jpg)\n",
    "* This is extremely important for regressions because we want a certain observation's inputs to match with its target even after shuffling."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Section 35: Advanced Statistical Methods - Practical Example: Linear Regression**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 225. Practical Example: Linear Regression (Part 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/learn/lecture/14390482#overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will deal with a real-life example about car sales.\n",
    "    * STEP BY STEP\n",
    "        * ![Alt text](img/225.%20Practical%20Example%20Linear%20Regression%20(Part%201)1.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 226. Practical Example: Linear Regression (Part 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 227. A Note on Multicollinearity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 228. Practical Example: Linear Regression (Part 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 229. Dummies and Variance Inflation Factor - Exercise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 230. Practical Example: Linear Regression (Part 4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 231. Dummy Variables - Exercise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 232. Practical Example: Linear Regression (Part 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 233. Linear Regression - Exercise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Section 36: Advanced Statistical Methods - Logistic Regression**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 234. Introduction to Logistic Regression\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 235. A Simple Example in Python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 236. Logistic vs Logit Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 237. Building a Logistic Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 238. Building a Logistic Regression - Exercise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 239. An Invaluable Coding Tip"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 240. Understanding Logistic Regression Tables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 241. Understanding Logistic Regression Tables - Exercise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 242. What do the Odds Actually Mean"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 243. Binary Predictors in a Logistic Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 244. Binary Predictors in a Logistic Regression - Exercise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 245. Calculating the Accuracy of the Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 246. Calculating the Accuracy of the Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 247. Underfitting and Overfitting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 248. Testing the Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 249. Testing the Model - Exercise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Section 37: Advanced Statistical Methods - Cluster Analysis**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 250. Introduction to Cluster Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 251. Some Examples of Clusters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 252. Difference between Classification and Clustering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 253. Math Prerequisites"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Section 38: Advanced Statistical Methods - K-Means Clustering**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 254. K-Means Clustering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 255. A Simple Example of Clustering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 256. A Simple Example of Clustering - Exercise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 257. Clustering Categorical Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 258. Clustering Categorical Data - Exercise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 259. How to Choose the Number of Clusters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 260. How to Choose the Number of Clusters - Exercise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 261. Pros and Cons of K-Means Clustering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 262. To Standardize or not to Standardize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 263. Relationship between Clustering and Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 264. Market Segmentation with Cluster Analysis (Part 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 265. Market Segmentation with Cluster Analysis (Part 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 266. How is Clustering Useful?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 267. EXERCISE: Species Segmentation with Cluster Analysis (Part 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 268. EXERCISE: Species Segmentation with Cluster Analysis (Part 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Section 39: Advanced Statistical Methods - Other Types of Clustering**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 269. Types of Clustering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 270. Dendrogram"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 271. Heatmaps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1b600a3438a79bc98971c3547d28f531d0c8ed6c91b96b1480a504fa30005dbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
